@article{arcomano_machine_2020,
	title = {A {Machine} {Learning}-{Based} {Global} {Atmospheric} {Forecast} {Model}},
	volume = {47},
	copyright = {©2020. American Geophysical Union. All Rights Reserved.},
	issn = {1944-8007},
	url = {http://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2020GL087776},
	doi = {10.1029/2020GL087776},
	abstract = {The paper investigates the applicability of machine learning (ML) to weather prediction by building a reservoir computing-based, low-resolution, global prediction model. The model is designed to take advantage of the massively parallel architecture of a modern supercomputer. The forecast performance of the model is assessed by comparing it to that of daily climatology, persistence, and a numerical (physics-based) model of identical prognostic state variables and resolution. Hourly resolution 20-day forecasts with the model predict realistic values of the atmospheric state variables at all forecast times for the entire globe. The ML model outperforms both climatology and persistence for the first three forecast days in the midlatitudes, but not in the tropics. Compared to the numerical model, the ML model performs best for the state variables most affected by parameterized processes in the numerical model.},
	language = {en},
	number = {9},
	journal = {Geophysical Research Letters},
	author = {Arcomano, Troy and Szunyogh, Istvan and Pathak, Jaideep and Wikner, Alexander and Hunt, Brian R. and Ott, Edward},
	year = {2020},
	pages = {e2020GL087776},
	annote = {e2020GL087776 10.1029/2020GL087776},
	file = {Arcomano_et_al_2020_A_Machine_Learning-Based_Global_Atmospheric_Forecast_Model.pdf:/Users/tsmith/Drive/zotero/Arcomano_et_al_2020_A_Machine_Learning-Based_Global_Atmospheric_Forecast_Model.pdf:application/pdf;Snapshot:/Users/tsmith/Zotero/storage/NCLPUTJ8/2020GL087776.html:text/html},
}

@article{dueben_challenges_2018,
	title = {Challenges and design choices for global weather and climate models based on machine learning},
	volume = {11},
	issn = {1991-959X},
	url = {https://gmd.copernicus.org/articles/11/3999/2018/},
	doi = {https://doi.org/10.5194/gmd-11-3999-2018},
	abstract = {{\textless}p{\textgreater}{\textless}strong class="journal-contentHeaderColor"{\textgreater}Abstract.{\textless}/strong{\textgreater} Can models that are based on deep learning and trained on atmospheric data compete with weather and climate models that are based on physical principles and the basic equations of motion? This question has been asked often recently due to the boom in deep-learning techniques. The question is valid given the huge amount of data that are available, the computational efficiency of deep-learning techniques and the limitations of today's weather and climate models in particular with respect to resolution and complexity.{\textless}/p{\textgreater}{\textless}p{\textgreater}In this paper, the question will be discussed in the context of global weather forecasts. A toy model for global weather predictions will be presented and used to identify challenges and fundamental design choices for a forecast system based on neural networks.{\textless}/p{\textgreater}},
	language = {English},
	number = {10},
	journal = {Geoscientific Model Development},
	author = {Dueben, Peter D. and Bauer, Peter},
	month = oct,
	year = {2018},
	note = {Publisher: Copernicus GmbH},
	pages = {3999--4009},
	file = {Dueben_Bauer_2018_Challenges_and_design_choices_for_global_weather_and_climate_models_based_on.pdf:/Users/tsmith/Drive/zotero/Dueben_Bauer_2018_Challenges_and_design_choices_for_global_weather_and_climate_models_based_on.pdf:application/pdf;Snapshot:/Users/tsmith/Zotero/storage/5TXEBIWF/2018.html:text/html},
}

@article{rasp_weatherbench_2020,
	title = {{WeatherBench}: {A} {Benchmark} {Data} {Set} for {Data}-{Driven} {Weather} {Forecasting}},
	volume = {12},
	copyright = {©2020. The Authors.},
	issn = {1942-2466},
	shorttitle = {{WeatherBench}},
	url = {http://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2020MS002203},
	doi = {https://doi.org/10.1029/2020MS002203},
	abstract = {Data-driven approaches, most prominently deep learning, have become powerful tools for prediction in many domains. A natural question to ask is whether data-driven methods could also be used to predict global weather patterns days in advance. First studies show promise but the lack of a common data set and evaluation metrics make intercomparison between studies difficult. Here we present a benchmark data set for data-driven medium-range weather forecasting (specifically 3–5 days), a topic of high scientific interest for atmospheric and computer scientists alike. We provide data derived from the ERA5 archive that has been processed to facilitate the use in machine learning models. We propose simple and clear evaluation metrics which will enable a direct comparison between different methods. Further, we provide baseline scores from simple linear regression techniques, deep learning models, as well as purely physical forecasting models. The data set is publicly available at https://github.com/pangeo-data/WeatherBench and the companion code is reproducible with tutorials for getting started. We hope that this data set will accelerate research in data-driven weather forecasting.},
	language = {en},
	number = {11},
	journal = {Journal of Advances in Modeling Earth Systems},
	author = {Rasp, Stephan and Dueben, Peter D. and Scher, Sebastian and Weyn, Jonathan A. and Mouatadid, Soukayna and Thuerey, Nils},
	year = {2020},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1029/2020MS002203},
	keywords = {machine learning, artificial intelligence, benchmark, NWP},
	pages = {e2020MS002203},
	annote = {e2020MS002203 10.1029/2020MS002203},
	file = {Rasp_et_al_2020_WeatherBench.pdf:/Users/tsmith/Drive/zotero/Rasp_et_al_2020_WeatherBench.pdf:application/pdf},
}

@article{nadiga_reservoir_2021,
	title = {Reservoir {Computing} as a {Tool} for {Climate} {Predictability} {Studies}},
	volume = {13},
	issn = {1942-2466},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1029/2020MS002290},
	doi = {10.1029/2020MS002290},
	abstract = {Reduced-order dynamical models play a central role in developing our understanding of predictability of climate irrespective of whether we are dealing with the actual climate system or surrogate climate models. In this context, the linear inverse modeling (LIM) approach, by capturing a few essential interactions between dynamical components of the full system, has proven valuable in providing insights into predictability of the full system. We demonstrate that reservoir computing (RC), a form of learning suitable for systems with chaotic dynamics, provides an alternative nonlinear approach that improves on the predictive skill of the LIM approach. We do this in the example setting of predicting sea surface temperature in the North Atlantic in the preindustrial control simulation of a popular earth system model, the Community Earth System Model so that we can compare the performance of the new RC-based approach with the traditional LIM approach both when learning data are plentiful and when such data are more limited. The improved predictive skill of the RC approach over a wide range of conditions—larger number of retained EOF coefficients, extending well into the limited data regime, etc.—suggests that this machine-learning technique may have a use in climate predictability studies. While the possibility of developing a climate emulator—the ability to continue the evolution of the system on the attractor long after failing to be able to track the reference trajectory—is demonstrated in the Lorenz-63 system, it is suggested that further development of the RC approach may permit such uses of the new approach in more realistic predictability studies.},
	language = {en},
	number = {4},
	journal = {Journal of Advances in Modeling Earth Systems},
	author = {Nadiga, Balasubramanya T.},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1029/2020MS002290},
	keywords = {machine learning, climate, echo state networks, reservoir computing, linear inverse modeling, predictability},
	pages = {e2020MS002290},
	annote = {e2020MS002290 2020MS002290},
	file = {Nadiga_2021_Reservoir_Computing_as_a_Tool_for_Climate_Predictability_Studies.pdf:/Users/tsmith/Drive/zotero/Nadiga_2021_Reservoir_Computing_as_a_Tool_for_Climate_Predictability_Studies.pdf:application/pdf;Nadiga_2021_Reservoir_Computing_as_a_Tool_for_Climate_Predictability_Studies.pdf:/Users/tsmith/Drive/zotero/Nadiga_2021_Reservoir_Computing_as_a_Tool_for_Climate_Predictability_Studies.pdf:application/pdf;Snapshot:/Users/tsmith/Zotero/storage/Q7D3QF8X/2020MS002290.html:text/html},
}

@article{agarwal_comparison_2021,
	title = {A {Comparison} of {Data}-{Driven} {Approaches} to {Build} {Low}-{Dimensional} {Ocean} {Models}},
	volume = {13},
	issn = {1942-2466},
	url = {http://onlinelibrary.wiley.com/doi/abs/10.1029/2021MS002537},
	doi = {10.1029/2021MS002537},
	abstract = {We present a comprehensive inter-comparison of linear regression (LR), stochastic, and deep-learning approaches for reduced-order statistical emulation of ocean circulation. The reference data set is provided by an idealized, eddy-resolving, double-gyre ocean circulation model. Our goal is to conduct a systematic and comprehensive assessment and comparison of skill, cost, and complexity of statistical models from the three methodological classes. The model based on LR is considered as a baseline. Additionally, we investigate its additive white noise augmentation and a multi-level stochastic approach, deep-learning methods, hybrid frameworks (LR plus deep-learning), and simple stochastic extensions of deep-learning and hybrid methods. The assessment metrics considered are: root mean squared error, anomaly cross-correlation, climatology, variance, frequency map, forecast horizon, and computational cost. We found that the multi-level linear stochastic approach performs the best for both short- and long-timescale forecasts. The deep-learning hybrid models augmented by additive state-dependent white noise came second, while their deterministic counterparts failed to reproduce the characteristic frequencies in climate-range forecasts. Pure deep learning implementations performed worse than LR and its simple white noise augmentation. Skills of LR and its white noise extension were similar on short timescales, but the latter performed better on long timescales, while LR-only outputs decay to zero for long simulations. Overall, our analysis promotes multi-level LR stochastic models with memory effects, and hybrid models with linear dynamical core augmented by additive stochastic terms learned via deep learning, as a more practical, accurate, and cost-effective option for ocean emulation than pure deep-learning solutions.},
	language = {en},
	number = {9},
	journal = {Journal of Advances in Modeling Earth Systems},
	author = {Agarwal, Niraj and Kondrashov, D. and Dueben, P. and Ryzhov, E. and Berloff, P.},
	year = {2021},
	note = {\_eprint: https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2021MS002537},
	keywords = {machine learning, data-driven modeling, ocean models, reduced order modeling},
	pages = {e2021MS002537},
	annote = {e2021MS002537 2021MS002537},
	file = {Agarwal_et_al_2021_A_Comparison_of_Data-Driven_Approaches_to_Build_Low-Dimensional_Ocean_Models.pdf:/Users/tsmith/Drive/zotero/Agarwal_et_al_2021_A_Comparison_of_Data-Driven_Approaches_to_Build_Low-Dimensional_Ocean_Models.pdf:application/pdf;Snapshot:/Users/tsmith/Zotero/storage/HPRD5JXJ/2021MS002537.html:text/html},
}

@article{chen_predicting_2021,
	title = {Predicting {Shallow} {Water} {Dynamics} using {Echo}-{State} {Networks} with {Transfer} {Learning}},
	url = {http://arxiv.org/abs/2112.09182},
	abstract = {In this paper we demonstrate that reservoir computing can be used to learn the dynamics of the shallow-water equations. In particular, while most previous applications of reservoir computing have required training on a particular trajectory to further predict the evolution along that trajectory alone, we show the capability of reservoir computing to predict trajectories of the shallow-water equations with initial conditions not seen in the training process. However, in this setting, we find that the performance of the network deteriorates for initial conditions with ambient conditions (such as total water height and average velocity) that are different from those in the training dataset. To circumvent this deficiency, we introduce a transfer learning approach wherein a small additional training step with the relevant ambient conditions is used to improve the predictions.},
	journal = {arXiv:2112.09182 [physics]},
	author = {Chen, Xiaoqian and Nadiga, Balasubramanya T. and Timofeyev, Ilya},
	month = dec,
	year = {2021},
	note = {arXiv: 2112.09182},
	keywords = {Physics - Geophysics, Computer Science - Machine Learning, 86-08, Physics - Data Analysis, Statistics and Probability},
	file = {arXiv.org Snapshot:/Users/tsmith/Zotero/storage/35L3BN69/2112.html:text/html;Chen_et_al_2021_Predicting_Shallow_Water_Dynamics_using_Echo-State_Networks_with_Transfer.pdf:/Users/tsmith/Drive/zotero/Chen_et_al_2021_Predicting_Shallow_Water_Dynamics_using_Echo-State_Networks_with_Transfer.pdf:application/pdf},
}

@article{keisler_forecasting_2022,
	title = {Forecasting {Global} {Weather} with {Graph} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2202.07575},
	abstract = {We present a data-driven approach for forecasting global weather using graph neural networks. The system learns to step forward the current 3D atmospheric state by six hours, and multiple steps are chained together to produce skillful forecasts going out several days into the future. The underlying model is trained on reanalysis data from ERA5 or forecast data from GFS. Test performance on metrics such as Z500 (geopotential height) and T850 (temperature) improves upon previous data-driven approaches and is comparable to operational, full-resolution, physical models from GFS and ECMWF, at least when evaluated on 1-degree scales and when using reanalysis initial conditions. We also show results from connecting this data-driven model to live, operational forecasts from GFS.},
	journal = {arXiv:2202.07575 [physics]},
	author = {Keisler, Ryan},
	month = feb,
	year = {2022},
	note = {arXiv: 2202.07575},
	keywords = {Physics - Atmospheric and Oceanic Physics, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/tsmith/Zotero/storage/TIIZMUZ4/2202.html:text/html;Keisler_2022_Forecasting_Global_Weather_with_Graph_Neural_Networks.pdf:/Users/tsmith/Drive/zotero/Keisler_2022_Forecasting_Global_Weather_with_Graph_Neural_Networks.pdf:application/pdf},
}

@techreport{lin_fourier_2021,
	type = {preprint},
	title = {Fourier {Reservoir} {Computing} for data-driven prediction of multi-scale coupled quasi-geostrophic dynamics},
	url = {http://www.essoar.org/doi/10.1002/essoar.10509867.1},
	language = {en},
	institution = {Geophysics},
	author = {Lin, Hsin-Yi and Penny, Stephen G},
	month = dec,
	year = {2021},
	doi = {10.1002/essoar.10509867.1},
	file = {Lin_Penny_2021_Fourier_Reservoir_Computing_for_data-driven_prediction_of_multi-scale_coupled.pdf:/Users/tsmith/Drive/zotero/Lin_Penny_2021_Fourier_Reservoir_Computing_for_data-driven_prediction_of_multi-scale_coupled.pdf:application/pdf},
}

@article{pathak_fourcastnet_2022,
	title = {{FourCastNet}: {A} {Global} {Data}-driven {High}-resolution {Weather} {Model} using {Adaptive} {Fourier} {Neural} {Operators}},
	shorttitle = {{FourCastNet}},
	url = {http://arxiv.org/abs/2202.11214},
	abstract = {FourCastNet, short for Fourier Forecasting Neural Network, is a global data-driven weather forecasting model that provides accurate short to medium-range global predictions at \$0.25{\textasciicircum}\{{\textbackslash}circ\}\$ resolution. FourCastNet accurately forecasts high-resolution, fast-timescale variables such as the surface wind speed, precipitation, and atmospheric water vapor. It has important implications for planning wind energy resources, predicting extreme weather events such as tropical cyclones, extra-tropical cyclones, and atmospheric rivers. FourCastNet matches the forecasting accuracy of the ECMWF Integrated Forecasting System (IFS), a state-of-the-art Numerical Weather Prediction (NWP) model, at short lead times for large-scale variables, while outperforming IFS for variables with complex fine-scale structure, including precipitation. FourCastNet generates a week-long forecast in less than 2 seconds, orders of magnitude faster than IFS. The speed of FourCastNet enables the creation of rapid and inexpensive large-ensemble forecasts with thousands of ensemble-members for improving probabilistic forecasting. We discuss how data-driven deep learning models such as FourCastNet are a valuable addition to the meteorology toolkit to aid and augment NWP models.},
	journal = {arXiv:2202.11214 [physics]},
	author = {Pathak, Jaideep and Subramanian, Shashank and Harrington, Peter and Raja, Sanjeev and Chattopadhyay, Ashesh and Mardani, Morteza and Kurth, Thorsten and Hall, David and Li, Zongyi and Azizzadenesheli, Kamyar and Hassanzadeh, Pedram and Kashinath, Karthik and Anandkumar, Animashree},
	month = feb,
	year = {2022},
	note = {arXiv: 2202.11214},
	keywords = {Computer Science - Machine Learning, Physics - Atmospheric and Oceanic Physics},
	file = {arXiv.org Snapshot:/Users/tsmith/Zotero/storage/UZLDZG3C/2202.html:text/html;Pathak_et_al_2022_FourCastNet.pdf:/Users/tsmith/Drive/zotero/Pathak_et_al_2022_FourCastNet.pdf:application/pdf},
}

@article{pandey_reservoir_2020,
	title = {Reservoir computing model of two-dimensional turbulent convection},
	volume = {5},
	url = {https://link.aps.org/doi/10.1103/PhysRevFluids.5.113506},
	doi = {10.1103/PhysRevFluids.5.113506},
	abstract = {Reservoir computing is an efficient implementation of a recurrent neural network that can describe the evolution of a dynamical system by supervised machine learning without solving the underlying mathematical equations. In this work, reservoir computing is applied to model the large-scale evolution and the resulting low-order turbulence statistics of a two-dimensional turbulent Rayleigh-Bénard convection flow at a Rayleigh number Ra=107 and a Prandtl number Pr=7 in an extended spatial domain with an aspect ratio of 6. Our data-driven approach, which is based on a long-term direct numerical simulation of the convection flow, comprises a two-step procedure: (1) reduction of the original simulation data by a proper orthogonal decomposition (POD) snapshot analysis and subsequent truncation to the first 150 POD modes which are associated with the largest total energy amplitudes; (2) setup and optimization of a reservoir computing model to describe the dynamical evolution of these 150 degrees of freedom and thus the large-scale evolution of the convection flow. The quality of the prediction of the reservoir computing model is comprehensively tested by a direct comparison of the results of the original direct numerical simulations and the fields that are reconstructed by means of the POD modes. We find a good agreement of the vertical profiles of mean temperature, mean convective heat flux, and root-mean-square temperature fluctuations. In addition, we discuss temperature variance spectra and joint probability density functions of the turbulent vertical velocity component and temperature fluctuation, the latter of which is essential for the turbulent heat transport across the layer. At the core of the model is the reservoir, a very large sparse random network characterized by the spectral radius of the corresponding adjacency matrix and a few further hyperparameters which are varied to investigate the quality of the prediction. Our work demonstrates that the reservoir computing model is capable of modeling the large-scale structure and low-order statistics of turbulent convection, which can open new avenues for modeling mesoscale convection processes in larger circulation models.},
	number = {11},
	journal = {Physical Review Fluids},
	author = {Pandey, Sandeep and Schumacher, Jörg},
	month = nov,
	year = {2020},
	note = {Publisher: American Physical Society},
	pages = {113506},
	file = {APS Snapshot:/Users/tsmith/Zotero/storage/6XFCT9XY/PhysRevFluids.5.html:text/html;Pandey_Schumacher_2020_Reservoir_computing_model_of_two-dimensional_turbulent_convection.pdf:/Users/tsmith/Drive/zotero/Pandey_Schumacher_2020_Reservoir_computing_model_of_two-dimensional_turbulent_convection.pdf:application/pdf},
}

@article{doan_short-_2021,
	title = {Short- and long-term predictions of chaotic flows and extreme events: a physics-constrained reservoir computing approach},
	volume = {477},
	shorttitle = {Short- and long-term predictions of chaotic flows and extreme events},
	url = {https://royalsocietypublishing.org/doi/full/10.1098/rspa.2021.0135},
	doi = {10.1098/rspa.2021.0135},
	abstract = {We propose a physics-constrained machine learning method—based on reservoir computing—to time-accurately predict extreme events and long-term velocity statistics in a model of chaotic flow. The method leverages the strengths of two different approaches: empirical modelling based on reservoir computing, which learns the chaotic dynamics from data only, and physical modelling based on conservation laws. This enables the reservoir computing framework to output physical predictions when training data are unavailable. We show that the combination of the two approaches is able to accurately reproduce the velocity statistics, and to predict the occurrence and amplitude of extreme events in a model of self-sustaining process in turbulence. In this flow, the extreme events are abrupt transitions from turbulent to quasi-laminar states, which are deterministic phenomena that cannot be traditionally predicted because of chaos. Furthermore, the physics-constrained machine learning method is shown to be robust with respect to noise. This work opens up new possibilities for synergistically enhancing data-driven methods with physical knowledge for the time-accurate prediction of chaotic flows.},
	number = {2253},
	journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Doan, N. a. K. and Polifke, W. and Magri, L.},
	month = sep,
	year = {2021},
	note = {Publisher: Royal Society},
	keywords = {machine learning, reservoir computing, chaotic flows, extreme events},
	pages = {20210135},
	file = {Doan_et_al_2021_Short-_and_long-term_predictions_of_chaotic_flows_and_extreme_events.pdf:/Users/tsmith/Drive/zotero/Doan_et_al_2021_Short-_and_long-term_predictions_of_chaotic_flows_and_extreme_events.pdf:application/pdf},
}

@article{moore_linear_2022,
	title = {A linear stochastic emulator of the {California} {Current} system using balanced truncation},
	volume = {174},
	issn = {14635003},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1463500322000610},
	doi = {10.1016/j.ocemod.2022.102023},
	abstract = {A downscaled reanalysis for the California Current system is used to construct a low-dimensional linear stochastic emulator of the 3-dimensional time evolving ocean circulation. The approach used is based on balanced truncation which simultaneously draws on information from the Empirical Orthogonal Functions (EOFs) and stochastic optimals of the ocean state-vector. In this way, balanced truncation faithfully preserves the inherent stability properties of the data, unlike the more traditional approaches based on truncation using EOFs alone. Since balanced truncation is predicated on singular value decomposition, formal error bounds on the accuracy of the reduced-dimension system can be computed. In addition, linear stochastic emulators that target different physical processes can also be constructed, and examples that focus on a region dominated by coastal upwelling are presented. Linear stochastic emulators, such as that developed here, can be exploited to generate very long simulations (or large ensembles) at high resolution that can be used to establish a statistical baseline for important oceanic processes, calculations that would otherwise be very challenging by direct numerical integration. An example is presented.},
	language = {en},
	journal = {Ocean Modelling},
	author = {Moore, Andrew M. and Fiechter, Jérôme and Edwards, Christopher A.},
	month = jun,
	year = {2022},
	pages = {102023},
	file = {Moore_et_al_2022_A_linear_stochastic_emulator_of_the_California_Current_system_using_balanced.pdf:/Users/tsmith/Drive/zotero/Moore_et_al_2022_A_linear_stochastic_emulator_of_the_California_Current_system_using_balanced.pdf:application/pdf},
}

@article{dueben_challenges_2022,
	title = {Challenges and {Benchmark} {Datasets} for {Machine} {Learning} in the {Atmospheric} {Sciences}: {Definition}, {Status}, and {Outlook}},
	volume = {1},
	issn = {2769-7525},
	shorttitle = {Challenges and {Benchmark} {Datasets} for {Machine} {Learning} in the {Atmospheric} {Sciences}},
	url = {https://journals.ametsoc.org/view/journals/aies/1/3/AIES-D-21-0002.1.xml},
	doi = {10.1175/AIES-D-21-0002.1},
	abstract = {Abstract Benchmark datasets and benchmark problems have been a key aspect for the success of modern machine learning applications in many scientific domains. Consequently, an active discussion about benchmarks for applications of machine learning has also started in the atmospheric sciences. Such benchmarks allow for the comparison of machine learning tools and approaches in a quantitative way and enable a separation of concerns for domain and machine learning scientists. However, a clear definition of benchmark datasets for weather and climate applications is missing with the result that many domain scientists are confused. In this paper, we equip the domain of atmospheric sciences with a recipe for how to build proper benchmark datasets, a (nonexclusive) list of domain-specific challenges for machine learning is presented, and it is elaborated where and what benchmark datasets will be needed to tackle these challenges. We hope that the creation of benchmark datasets will help the machine learning efforts in atmospheric sciences to be more coherent, and, at the same time, target the efforts of machine learning scientists and experts of high-performance computing to the most imminent challenges in atmospheric sciences. We focus on benchmarks for atmospheric sciences (weather, climate, and air-quality applications). However, many aspects of this paper will also hold for other aspects of the Earth system sciences or are at least transferable. Significance Statement Machine learning is the study of computer algorithms that learn automatically from data. Atmospheric sciences have started to explore sophisticated machine learning techniques and the community is making rapid progress on the uptake of new methods for a large number of application areas. This paper provides a clear definition of so-called benchmark datasets for weather and climate applications that help to share data and machine learning solutions between research groups to reduce time spent in data processing, to generate synergies between groups, and to make tool developments more targeted and comparable. Furthermore, a list of benchmark datasets that will be needed to tackle important challenges for the use of machine learning in atmospheric sciences is provided.},
	language = {EN},
	number = {3},
	journal = {Artificial Intelligence for the Earth Systems},
	author = {Dueben, Peter D. and Schultz, Martin G. and Chantry, Matthew and Gagne, David John and Hall, David Matthew and McGovern, Amy},
	month = jul,
	year = {2022},
	note = {Publisher: American Meteorological Society
Section: Artificial Intelligence for the Earth Systems},
	file = {Dueben_et_al_2022_Challenges_and_Benchmark_Datasets_for_Machine_Learning_in_the_Atmospheric.pdf:/Users/tsmith/Drive/zotero/Dueben_et_al_2022_Challenges_and_Benchmark_Datasets_for_Machine_Learning_in_the_Atmospheric.pdf:application/pdf;Snapshot:/Users/tsmith/Zotero/storage/M8WL4X3I/AIES-D-21-0002.1.html:text/html},
}

@article{brunton_machine_2020,
	title = {Machine {Learning} for {Fluid} {Mechanics}},
	volume = {52},
	url = {https://doi.org/10.1146/annurev-fluid-010719-060214},
	doi = {10.1146/annurev-fluid-010719-060214},
	abstract = {The field of fluid mechanics is rapidly advancing, driven by unprecedented volumes of data from experiments, field measurements, and large-scale simulations at multiple spatiotemporal scales. Machine learning (ML) offers a wealth of techniques to extract information from data that can be translated into knowledge about the underlying fluid mechanics. Moreover, ML algorithms can augment domain knowledge and automate tasks related to flow control and optimization. This article presents an overview of past history, current developments, and emerging opportunities of ML for fluid mechanics. We outline fundamental ML methodologies and discuss their uses for understanding, modeling, optimizing, and controlling fluid flows. The strengths and limitations of these methods are addressed from the perspective of scientific inquiry that considers data as an inherent part of modeling, experiments, and simulations. ML provides a powerful information-processing framework that can augment, and possibly even transform, current lines of fluid mechanics research and industrial applications.},
	number = {1},
	journal = {Annual Review of Fluid Mechanics},
	author = {Brunton, Steven L. and Noack, Bernd R. and Koumoutsakos, Petros},
	year = {2020},
	note = {\_eprint: https://doi.org/10.1146/annurev-fluid-010719-060214},
	keywords = {machine learning, optimization, data-driven modeling, control},
	pages = {477--508},
	file = {Brunton_et_al_2020_Machine_Learning_for_Fluid_Mechanics.pdf:/Users/tsmith/Drive/zotero/Brunton_et_al_2020_Machine_Learning_for_Fluid_Mechanics.pdf:application/pdf},
}

@article{chattopadhyay_deep_2020,
	title = {Deep spatial transformers for autoregressive data-driven forecasting of geophysical turbulence},
	copyright = {GNU Lesser General Public License (LGPL) 2.1},
	url = {https://eartharxiv.org/repository/view/118/},
	abstract = {A deep spatial transformer based encoder-decoder model has been developed to autoregressively predict the time evolution of the upper layers stream function of a two-layered quasi-geostrophic (QG) system without any information about the lower layers stream function. The spatio-temporal complexity of QG flow is comparable to the complexity of 500hPa Geopotential Height (Z500) of fully coupled climate models or even the Z500 which is observed in the atmosphere, based on the instantaneous attractor dimension metric. The ability to predict autoregressively, the turbulent dynamics of QG is the first step towards building data-driven surrogates for more complex climate models. We show that the equivariance preserving properties of modern spatial transformers incorporated within a convolutional encoder-decoder module can predict up to 9 days in a QG system (outperforming a baseline persistence model and a standard convolutional encoder decoder with a custom loss function). The proposed data-driven model remains stable for multiple years thus promising us of a stable and physical data-driven climate model.},
	language = {en},
	author = {Chattopadhyay, Ashesh and Mustafa, Mustafa and Hassanzadeh, Pedram and Kashinath, Karthik},
	month = jul,
	year = {2020},
	note = {Publisher: EarthArXiv},
    journal = {EarthArXiv},
	file = {Chattopadhyay_et_al_2020_Deep_spatial_transformers_for_autoregressive_data-driven_forecasting_of.pdf:/Users/tsmith/Drive/zotero/Chattopadhyay_et_al_2020_Deep_spatial_transformers_for_autoregressive_data-driven_forecasting_of.pdf:application/pdf;Snapshot:/Users/tsmith/Zotero/storage/7FRV5NT3/118.html:text/html},
}

@misc{kurth_fourcastnet_2022,
	title = {{FourCastNet}: {Accelerating} {Global} {High}-{Resolution} {Weather} {Forecasting} using {Adaptive} {Fourier} {Neural} {Operators}},
	shorttitle = {{FourCastNet}},
	url = {http://arxiv.org/abs/2208.05419},
	abstract = {Extreme weather amplified by climate change is causing increasingly devastating impacts across the globe. The current use of physics-based numerical weather prediction (NWP) limits accuracy due to high computational cost and strict time-to-solution limits. We report that a data-driven deep learning Earth system emulator, FourCastNet, can predict global weather and generate medium-range forecasts five orders-of-magnitude faster than NWP while approaching state-of-the-art accuracy. FourCast-Net is optimized and scales efficiently on three supercomputing systems: Selene, Perlmutter, and JUWELS Booster up to 3,808 NVIDIA A100 GPUs, attaining 140.8 petaFLOPS in mixed precision (11.9\%of peak at that scale). The time-to-solution for training FourCastNet measured on JUWELS Booster on 3,072GPUs is 67.4minutes, resulting in an 80,000times faster time-to-solution relative to state-of-the-art NWP, in inference. FourCastNet produces accurate instantaneous weather predictions for a week in advance, enables enormous ensembles that better capture weather extremes, and supports higher global forecast resolutions.},
	publisher = {arXiv},
	author = {Kurth, Thorsten and Subramanian, Shashank and Harrington, Peter and Pathak, Jaideep and Mardani, Morteza and Hall, David and Miele, Andrea and Kashinath, Karthik and Anandkumar, Animashree},
	month = aug,
	year = {2022},
	note = {arXiv:2208.05419 [physics]},
	keywords = {Physics - Atmospheric and Oceanic Physics, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Performance},
	file = {arXiv.org Snapshot:/Users/tsmith/Zotero/storage/U78S3MRL/2208.html:text/html;Kurth_et_al_2022_FourCastNet.pdf:/Users/tsmith/Drive/zotero/Kurth_et_al_2022_FourCastNet.pdf:application/pdf},
}

@article{furner_sensitivity_2022,
	title = {A sensitivity analysis of a regression model of ocean temperature},
	volume = {1},
	issn = {2634-4602},
	url = {https://www.cambridge.org/core/journals/environmental-data-science/article/sensitivity-analysis-of-a-regression-model-of-ocean-temperature/089EA5B347F262CEE0B4BDFFBA4E6FF5},
	doi = {10.1017/eds.2022.10},
	abstract = {There has been much recent interest in developing data-driven models for weather and climate predictions. However, there are open questions regarding their generalizability and robustness, highlighting a need to better understand how they make their predictions. In particular, it is important to understand whether data-driven models learn the underlying physics of the system against which they are trained, or simply identify statistical patterns without any clear link to the underlying physics. In this paper, we describe a sensitivity analysis of a regression-based model of ocean temperature, trained against simulations from a 3D ocean model setup in a very simple configuration. We show that the regressor heavily bases its forecasts on, and is dependent on, variables known to be key to the physics such as currents and density. By contrast, the regressor does not make heavy use of inputs such as location, which have limited direct physical impacts. The model requires nonlinear interactions between inputs in order to show any meaningful skill—in line with the highly nonlinear dynamics of the ocean. Further analysis interprets the ways certain variables are used by the regression model. We see that information about the vertical profile of the water column reduces errors in regions of convective activity, and information about the currents reduces errors in regions dominated by advective processes. Our results demonstrate that even a simple regression model is capable of learning much of the physics of the system being modeled. We expect that a similar sensitivity analysis could be usefully applied to more complex ocean configurations.},
	language = {en},
	journal = {Environmental Data Science},
	author = {Furner, Rachel and Haynes, Peter and Munday, Dave and Paige, Brooks and Jones, Daniel C. and Shuckburgh, Emily},
	year = {2022},
	note = {Publisher: Cambridge University Press},
	keywords = {oceanography, Data science, interpretable ML, model sensitivity, regression model},
	pages = {e11},
	file = {Furner_et_al_2022_A_sensitivity_analysis_of_a_regression_model_of_ocean_temperature.pdf:/Users/tsmith/Drive/zotero/Furner_et_al_2022_A_sensitivity_analysis_of_a_regression_model_of_ocean_temperature.pdf:application/pdf},
}

@article{weyn_sub-seasonal_2021,
	title = {Sub-{Seasonal} {Forecasting} {With} a {Large} {Ensemble} of {Deep}-{Learning} {Weather} {Prediction} {Models}},
	volume = {13},
	issn = {1942-2466},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1029/2021MS002502},
	doi = {10.1029/2021MS002502},
	abstract = {We present an ensemble prediction system using a Deep Learning Weather Prediction (DLWP) model that recursively predicts six key atmospheric variables with six-hour time resolution. This computationally efficient model uses convolutional neural networks (CNNs) on a cubed sphere grid to produce global forecasts. The trained model requires just three minutes on a single GPU to produce a 320-member set of six-week forecasts at 1.4° resolution. Ensemble spread is primarily produced by randomizing the CNN training process to create a set of 32 DLWP models with slightly different learned weights. Although our DLWP model does not forecast precipitation, it does forecast total column water vapor and gives a reasonable 4.5-day deterministic forecast of Hurricane Irma. In addition to simulating mid-latitude weather systems, it spontaneously generates tropical cyclones in a one-year free-running simulation. Averaged globally and over a two-year test set, the ensemble mean RMSE retains skill relative to climatology beyond two-weeks, with anomaly correlation coefficients remaining above 0.6 through six days. Our primary application is to subseasonal-to-seasonal (S2S) forecasting at lead times from two to six weeks. Current forecast systems have low skill in predicting one- or 2-week-average weather patterns at S2S time scales. The continuous ranked probability score (CRPS) and the ranked probability skill score (RPSS) show that the DLWP ensemble is only modestly inferior in performance to the European Center for Medium Range Weather Forecasts (ECMWF) S2S ensemble over land at lead times of 4 and 5–6 weeks. At shorter lead times, the ECMWF ensemble performs better than DLWP.},
	language = {en},
	number = {7},
	journal = {Journal of Advances in Modeling Earth Systems},
	author = {Weyn, Jonathan A. and Durran, Dale R. and Caruana, Rich and Cresswell-Clay, Nathaniel},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1029/2021MS002502},
	keywords = {machine learning, Deep learning, NWP, S2S},
	pages = {e2021MS002502},
	annote = {e2021MS002502 2021MS002502},
	file = {Snapshot:/Users/tsmith/Zotero/storage/CEGLP337/2021MS002502.html:text/html;Weyn_et_al_2021_Sub-Seasonal_Forecasting_With_a_Large_Ensemble_of_Deep-Learning_Weather.pdf:/Users/tsmith/Drive/zotero/Weyn_et_al_2021_Sub-Seasonal_Forecasting_With_a_Large_Ensemble_of_Deep-Learning_Weather.pdf:application/pdf},
}

@article{weyn_improving_2020,
	title = {Improving {Data}-{Driven} {Global} {Weather} {Prediction} {Using} {Deep} {Convolutional} {Neural} {Networks} on a {Cubed} {Sphere}},
	volume = {12},
	issn = {1942-2466},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1029/2020MS002109},
	doi = {10.1029/2020MS002109},
	abstract = {We present a significantly improved data-driven global weather forecasting framework using a deep convolutional neural network (CNN) to forecast several basic atmospheric variables on a global grid. New developments in this framework include an off-line volume-conservative mapping to a cubed-sphere grid, improvements to the CNN architecture and the minimization of the loss function over multiple steps in a prediction sequence. The cubed-sphere remapping minimizes the distortion on the cube faces on which convolution operations are performed and provides natural boundary conditions for padding in the CNN. Our improved model produces weather forecasts that are indefinitely stable and produce realistic weather patterns at lead times of several weeks and longer. For short- to medium-range forecasting, our model significantly outperforms persistence, climatology, and a coarse-resolution dynamical numerical weather prediction (NWP) model. Unsurprisingly, our forecasts are worse than those from a high-resolution state-of-the-art operational NWP system. Our data-driven model is able to learn to forecast complex surface temperature patterns from few input atmospheric state variables. On annual time scales, our model produces a realistic seasonal cycle driven solely by the prescribed variation in top-of-atmosphere solar forcing. Although it currently does not compete with operational weather forecasting models, our data-driven CNN executes much faster than those models, suggesting that machine learning could prove to be a valuable tool for large-ensemble forecasting.},
	language = {en},
	number = {9},
	journal = {Journal of Advances in Modeling Earth Systems},
	author = {Weyn, Jonathan A. and Durran, Dale R. and Caruana, Rich},
	year = {2020},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1029/2020MS002109},
	pages = {e2020MS002109},
	annote = {e2020MS002109 10.1029/2020MS002109},
	file = {Snapshot:/Users/tsmith/Zotero/storage/X2BALKJA/2020MS002109.html:text/html;Weyn_et_al_2020_Improving_Data-Driven_Global_Weather_Prediction_Using_Deep_Convolutional_Neural.pdf:/Users/tsmith/Drive/zotero/Weyn_et_al_2020_Improving_Data-Driven_Global_Weather_Prediction_Using_Deep_Convolutional_Neural.pdf:application/pdf},
}

@article{scher_weather_2019,
	title = {Weather and climate forecasting with neural networks: using general circulation models ({GCMs}) with different complexity as a study ground},
	volume = {12},
	issn = {1991-959X},
	shorttitle = {Weather and climate forecasting with neural networks},
	url = {https://gmd.copernicus.org/articles/12/2797/2019/},
	doi = {10.5194/gmd-12-2797-2019},
	abstract = {{\textless}p{\textgreater}{\textless}strong class="journal-contentHeaderColor"{\textgreater}Abstract.{\textless}/strong{\textgreater} Recently, there has been growing interest in the possibility of using neural networks for both weather forecasting and the generation of climate datasets. We use a bottom–up approach for assessing whether it should, in principle, be possible to do this. We use the relatively simple general circulation models (GCMs) PUMA and PLASIM as a simplified reality on which we train deep neural networks, which we then use for predicting the model weather at lead times of a few days. We specifically assess how the complexity of the climate model affects the neural network's forecast skill and how dependent the skill is on the length of the provided training period. Additionally, we show that using the neural networks to reproduce the climate of general circulation models including a seasonal cycle remains challenging – in contrast to earlier promising results on a model without seasonal cycle.{\textless}/p{\textgreater}},
	language = {English},
	number = {7},
	journal = {Geoscientific Model Development},
	author = {Scher, Sebastian and Messori, Gabriele},
	month = jul,
	year = {2019},
	note = {Publisher: Copernicus GmbH},
	pages = {2797--2809},
	file = {Scher_Messori_2019_Weather_and_climate_forecasting_with_neural_networks.pdf:/Users/tsmith/Drive/zotero/Scher_Messori_2019_Weather_and_climate_forecasting_with_neural_networks.pdf:application/pdf;Snapshot:/Users/tsmith/Zotero/storage/LQUNMMG8/gmd-12-2797-2019-discussion.html:text/html},
}

@article{weyn_can_2019,
	title = {Can {Machines} {Learn} to {Predict} {Weather}? {Using} {Deep} {Learning} to {Predict} {Gridded} 500-{hPa} {Geopotential} {Height} {From} {Historical} {Weather} {Data}},
	volume = {11},
	issn = {1942-2466},
	shorttitle = {Can {Machines} {Learn} to {Predict} {Weather}?},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1029/2019MS001705},
	doi = {10.1029/2019MS001705},
	abstract = {We develop elementary weather prediction models using deep convolutional neural networks (CNNs) trained on past weather data to forecast one or two fundamental meteorological fields on a Northern Hemisphere grid with no explicit knowledge about physical processes. At forecast lead times up to 3 days, CNNs trained to predict only 500-hPa geopotential height easily outperform persistence, climatology, and the dynamics-based barotropic vorticity model, but do not beat an operational full-physics weather prediction model. These CNNs are capable of forecasting significant changes in the intensity of weather systems, which is notable because this is beyond the capability of the fundamental dynamical equation that relies solely on 500-hPa data, the barotropic vorticity equation. Modest improvements to the CNN forecasts can be made by adding 700- to 300-hPa thickness to the input data. Our best performing CNN does a good job of capturing the climatology and annual variability of 500-hPa heights and is capable of forecasting realistic atmospheric states at lead times of 14 days. Although our simple models do not perform better than an operational weather model, machine learning warrants further exploration as a weather forecasting tool; in particular, the potential efficiency of CNNs might make them attractive for ensemble forecasting.},
	language = {en},
	number = {8},
	journal = {Journal of Advances in Modeling Earth Systems},
	author = {Weyn, Jonathan A. and Durran, Dale R. and Caruana, Rich},
	year = {2019},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1029/2019MS001705},
	keywords = {machine learning, weather prediction, deep learning, neural network},
	pages = {2680--2693},
	file = {Snapshot:/Users/tsmith/Zotero/storage/FRCJ3UNE/2019MS001705.html:text/html;Weyn_et_al_2019_Can_Machines_Learn_to_Predict_Weather.pdf:/Users/tsmith/Drive/zotero/Weyn_et_al_2019_Can_Machines_Learn_to_Predict_Weather.pdf:application/pdf},
}

@inproceedings{shi_convolutional_2015,
	title = {Convolutional {LSTM} {Network}: {A} {Machine} {Learning} {Approach} for {Precipitation} {Nowcasting}},
	volume = {28},
	shorttitle = {Convolutional {LSTM} {Network}},
	url = {https://proceedings.neurips.cc/paper/2015/hash/07563a3fe3bbe7e3ba84431ad9d055af-Abstract.html},
	abstract = {The goal of precipitation nowcasting is to predict the future rainfall intensity in a local region over a relatively short period of time. Very few previous studies have examined this crucial and challenging weather forecasting problem from the machine learning perspective. In this paper, we formulate precipitation nowcasting as a spatiotemporal sequence forecasting problem in which both the input and the prediction target are spatiotemporal sequences. By extending the fully connected LSTM (FC-LSTM) to have convolutional structures in both the input-to-state and state-to-state transitions, we propose the convolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable model for the precipitation nowcasting problem. Experiments show that our ConvLSTM network captures spatiotemporal correlations better and consistently outperforms FC-LSTM and the state-of-the-art operational ROVER algorithm for precipitation nowcasting.},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {SHI, Xingjian and Chen, Zhourong and Wang, Hao and Yeung, Dit-Yan and Wong, Wai-kin and WOO, Wang-chun},
	year = {2015},
	file = {SHI_et_al_2015_Convolutional_LSTM_Network.pdf:/Users/tsmith/Drive/zotero/SHI_et_al_2015_Convolutional_LSTM_Network.pdf:application/pdf},
}

@article{rasp_data-driven_2021,
	title = {Data-{Driven} {Medium}-{Range} {Weather} {Prediction} {With} a {Resnet} {Pretrained} on {Climate} {Simulations}: {A} {New} {Model} for {WeatherBench}},
	volume = {13},
	issn = {1942-2466},
	shorttitle = {Data-{Driven} {Medium}-{Range} {Weather} {Prediction} {With} a {Resnet} {Pretrained} on {Climate} {Simulations}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1029/2020MS002405},
	doi = {10.1029/2020MS002405},
	abstract = {Numerical weather prediction has traditionally been based on the models that discretize the dynamical and physical equations of the atmosphere. Recently, however, the rise of deep learning has created increased interest in purely data-driven medium-range weather forecasting with first studies exploring the feasibility of such an approach. To accelerate progress in this area, the WeatherBench benchmark challenge was defined. Here, we train a deep residual convolutional neural network (Resnet) to predict geopotential, temperature and precipitation at 5.625° resolution up to 5 days ahead. To avoid overfitting and improve forecast skill, we pretrain the model using historical climate model output before fine-tuning on reanalysis data. The resulting forecasts outperform previous submissions to WeatherBench and are comparable in skill to a physical baseline at similar resolution. We also analyze how the neural network creates its predictions and find that, for the case studies analyzed, the model has learned physically reasonable correlations. Finally, we perform scaling experiments to estimate the potential skill of data-driven approaches at higher resolutions.},
	language = {en},
	number = {2},
	journal = {Journal of Advances in Modeling Earth Systems},
	author = {Rasp, Stephan and Thuerey, Nils},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1029/2020MS002405},
	keywords = {machine learning, deep learning, numerical weather forecasting},
	pages = {e2020MS002405},
	annote = {e2020MS002405 2020MS002405},
	file = {Rasp_Thuerey_2021_Data-Driven_Medium-Range_Weather_Prediction_With_a_Resnet_Pretrained_on_Climate.pdf:/Users/tsmith/Drive/zotero/Rasp_Thuerey_2021_Data-Driven_Medium-Range_Weather_Prediction_With_a_Resnet_Pretrained_on_Climate.pdf:application/pdf;Snapshot:/Users/tsmith/Zotero/storage/T3N9E3J5/2020MS002405.html:text/html},
}

@article{clare_combining_2021,
	title = {Combining distribution-based neural networks to predict weather forecast probabilities},
	volume = {147},
	issn = {1477-870X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/qj.4180},
	doi = {10.1002/qj.4180},
	abstract = {The success of deep learning techniques over the last decades has opened up a new avenue of research for weather forecasting. Here, we take the novel approach of using a neural network to predict full probability density functions at each point in space and time rather than a single output value, thus producing a probabilistic weather forecast. This enables the calculation of both uncertainty and skill metrics for the neural network predictions, and overcomes the common difficulty of inferring uncertainty from these predictions. This approach is data-driven and the neural network is trained on the WeatherBench dataset (processed ERA5 data) to forecast geopotential and temperature 3 and 5 days ahead. Data exploration leads to the identification of the most important input variables. In order to increase computational efficiency, several neural networks are trained on small subsets of these variables. The outputs are then combined through a stacked neural network, the first time such a technique has been applied to weather data. Our approach is found to be more accurate than some coarse numerical weather prediction models and as accurate as more complex alternative neural networks, with the added benefit of providing key probabilistic information necessary for making informed weather forecasts.},
	language = {en},
	number = {741},
	journal = {Quarterly Journal of the Royal Meteorological Society},
	author = {Clare, Mariana C.A. and Jamil, Omar and Morcrette, Cyril J.},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/qj.4180},
	keywords = {deep learning, data exploration, ensemble dropout, probabilistic weather forecasting, probability density functions, ResNet, stacked neural network},
	pages = {4337--4357},
	file = {Clare_et_al_2021_Combining_distribution-based_neural_networks_to_predict_weather_forecast.pdf:/Users/tsmith/Drive/zotero/Clare_et_al_2021_Combining_distribution-based_neural_networks_to_predict_weather_forecast.pdf:application/pdf;Snapshot:/Users/tsmith/Zotero/storage/L7MLLTLD/qj.html:text/html},
}

@article{faranda_enhancing_2021,
	title = {Enhancing geophysical flow machine learning performance via scale separation},
	volume = {28},
	issn = {1023-5809},
	url = {https://npg.copernicus.org/articles/28/423/2021/},
	doi = {10.5194/npg-28-423-2021},
	abstract = {{\textless}p{\textgreater}{\textless}strong class="journal-contentHeaderColor"{\textgreater}Abstract.{\textless}/strong{\textgreater} Recent advances in statistical and machine learning have opened the possibility of forecasting the behaviour of chaotic systems using recurrent neural networks. In this article we investigate the applicability of such a framework to geophysical flows, known to involve multiple scales in length, time and energy and to feature intermittency. We show that both multiscale dynamics and intermittency introduce severe limitations to the applicability of recurrent neural networks, both for short-term forecasts as well as for the reconstruction of the underlying attractor. We suggest that possible strategies to overcome such limitations should be based on separating the smooth large-scale dynamics from the intermittent/small-scale features. We test these ideas on global sea-level pressure data for the past 40 years, a proxy of the atmospheric circulation dynamics. Better short- and long-term forecasts of sea-level pressure data can be obtained with an optimal choice of spatial coarse graining and time filtering.{\textless}/p{\textgreater}},
	language = {English},
	number = {3},
	journal = {Nonlinear Processes in Geophysics},
	author = {Faranda, Davide and Vrac, Mathieu and Yiou, Pascal and Pons, Flavio Maria Emanuele and Hamid, Adnane and Carella, Giulia and Ngoungue Langue, Cedric and Thao, Soulivanh and Gautard, Valerie},
	month = sep,
	year = {2021},
	note = {Publisher: Copernicus GmbH},
	pages = {423--443},
	file = {Faranda_et_al_2021_Enhancing_geophysical_flow_machine_learning_performance_via_scale_separation.pdf:/Users/tsmith/Drive/zotero/Faranda_et_al_2021_Enhancing_geophysical_flow_machine_learning_performance_via_scale_separation.pdf:application/pdf;Snapshot:/Users/tsmith/Zotero/storage/ZZ3HHTKT/2021.html:text/html},
}

@article{maulik_efficient_2022,
	title = {Efficient high-dimensional variational data assimilation with machine-learned reduced-order models},
	volume = {15},
	issn = {1991-9603},
	url = {https://gmd.copernicus.org/articles/15/3433/2022/},
	doi = {10.5194/gmd-15-3433-2022},
	abstract = {Data assimilation (DA) in geophysical sciences remains the cornerstone of robust forecasts from numerical models. Indeed, DA plays a crucial role in the quality of numerical weather prediction and is a crucial building block that has allowed dramatic improvements in weather forecasting over the past few decades. DA is commonly framed in a variational setting, where one solves an optimization problem within a Bayesian formulation using raw model forecasts as a prior and observations as likelihood. This leads to a DA objective function that needs to be minimized, where the decision variables are the initial conditions speciﬁed to the model. In traditional DA, the forward model is numerically and computationally expensive. Here we replace the forward model with a low-dimensional, data-driven, and differentiable emulator. Consequently, gradients of our DA objective function with respect to the decision variables are obtained rapidly via automatic differentiation. We demonstrate our approach by performing an emulator-assisted DA forecast of geopotential height. Our results indicate that emulatorassisted DA is faster than traditional equation-based DA forecasts by 4 orders of magnitude, allowing computations to be performed on a workstation rather than a dedicated highperformance computer. In addition, we describe accuracy beneﬁts of emulator-assisted DA when compared to simply using the emulator for forecasting (i.e., without DA). Our overall formulation is denoted AIEADA (Artiﬁcial Intelligence Emulator-Assisted Data Assimilation).},
	language = {en},
	number = {8},
	journal = {Geoscientific Model Development},
	author = {Maulik, Romit and Rao, Vishwas and Wang, Jiali and Mengaldo, Gianmarco and Constantinescu, Emil and Lusch, Bethany and Balaprakash, Prasanna and Foster, Ian and Kotamarthi, Rao},
	month = may,
	year = {2022},
	pages = {3433--3445},
	file = {Maulik_et_al_2022_Efficient_high-dimensional_variational_data_assimilation_with_machine-learned.pdf:/Users/tsmith/Drive/zotero/Maulik_et_al_2022_Efficient_high-dimensional_variational_data_assimilation_with_machine-learned.pdf:application/pdf},
}

@article{nonnenmacher_deep_2021,
	title = {Deep {Emulators} for {Differentiation}, {Forecasting}, and {Parametrization} in {Earth} {Science} {Simulators}},
	volume = {13},
	issn = {1942-2466},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1029/2021MS002554},
	doi = {10.1029/2021MS002554},
	abstract = {To understand and predict large, complex, and chaotic systems, Earth scientists build simulators from physical laws. Simulators generalize better to new scenarios, require fewer tunable parameters, and are more interpretable than nonphysical deep learning, but procedures for obtaining their derivatives with respect to their inputs are often unavailable. These missing derivatives limit the application of many important tools for forecasting, model tuning, sensitivity analysis, or subgrid-scale parametrization. Here, we propose to overcome this limitation with deep emulator networks that learn to calculate the missing derivatives. By training directly on simulation data without analyzing source code or equations, this approach supports simulators in any programming language on any hardware without specialized routines for each case. To demonstrate the effectiveness of our approach, we train emulators on complete or partial system states of the chaotic Lorenz-96 simulator and evaluate the accuracy of their dynamics and derivatives as a function of integration time and training data set size. We further demonstrate that emulator-derived derivatives enable accurate 4D-Var data assimilation and closed-loop training of parametrizations. These results provide a basis for further combining the parsimony and generality of physical models with the power and flexibility of machine learning.},
	language = {en},
	number = {7},
	journal = {Journal of Advances in Modeling Earth Systems},
	author = {Nonnenmacher, Marcel and Greenberg, David S.},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1029/2021MS002554},
	keywords = {machine learning, data assimilation, Lorenz-96, deep learning, model Jacobians, parametrization tuning},
	pages = {e2021MS002554},
	annote = {e2021MS002554 2021MS002554},
	file = {Nonnenmacher_Greenberg_2021_Deep_Emulators_for_Differentiation,_Forecasting,_and_Parametrization_in_Earth.pdf:/Users/tsmith/Drive/zotero/Nonnenmacher_Greenberg_2021_Deep_Emulators_for_Differentiation,_Forecasting,_and_Parametrization_in_Earth.pdf:application/pdf;Snapshot:/Users/tsmith/Zotero/storage/QDG5DWSI/2021MS002554.html:text/html},
}

@article{schultz_can_2021,
	title = {Can deep learning beat numerical weather prediction?},
	volume = {379},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsta.2020.0097},
	doi = {10.1098/rsta.2020.0097},
	abstract = {The recent hype about artificial intelligence has sparked renewed interest in applying the successful deep learning (DL) methods for image recognition, speech recognition, robotics, strategic games and other application areas to the field of meteorology. There is some evidence that better weather forecasts can be produced by introducing big data mining and neural networks into the weather prediction workflow. Here, we discuss the question of whether it is possible to completely replace the current numerical weather models and data assimilation systems with DL approaches. This discussion entails a review of state-of-the-art machine learning concepts and their applicability to weather data with its pertinent statistical properties. We think that it is not inconceivable that numerical weather models may one day become obsolete, but a number of fundamental breakthroughs are needed before this goal comes into reach.

This article is part of the theme issue ‘Machine learning for weather and climate modelling’.},
	number = {2194},
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Schultz, M. G. and Betancourt, C. and Gong, B. and Kleinert, F. and Langguth, M. and Leufen, L. H. and Mozaffari, A. and Stadtler, S.},
	month = apr,
	year = {2021},
	note = {Publisher: Royal Society},
	keywords = {machine learning, deep learning, numerical weather prediction, spatiotemporal pattern recognition, weather AI},
	pages = {20200097},
	file = {Schultz_et_al_2021_Can_deep_learning_beat_numerical_weather_prediction.pdf:/Users/tsmith/Drive/zotero/Schultz_et_al_2021_Can_deep_learning_beat_numerical_weather_prediction.pdf:application/pdf},
}


@article{scher_toward_2018,
	title = {Toward {Data}-{Driven} {Weather} and {Climate} {Forecasting}: {Approximating} a {Simple} {General} {Circulation} {Model} {With} {Deep} {Learning}},
	volume = {45},
	issn = {1944-8007},
	shorttitle = {Toward {Data}-{Driven} {Weather} and {Climate} {Forecasting}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1029/2018GL080704},
	doi = {10.1029/2018GL080704},
	abstract = {It is shown that it is possible to emulate the dynamics of a simple general circulation model with a deep neural network. After being trained on the model, the network can predict the complete model state several time steps ahead—which conceptually is making weather forecasts in the model world. Additionally, after being initialized with an arbitrary model state, the network can through repeatedly feeding back its predictions into its inputs create a climate run, which has similar climate statistics to the climate of the general circulation model. This network climate run shows no long-term drift, even though no conservation properties were explicitly designed into the network.},
	language = {en},
	number = {22},
	journal = {Geophysical Research Letters},
	author = {Scher, S.},
	year = {2018},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1029/2018GL080704},
	keywords = {climate models, deep learning, machine learning, neural networks, weather prediction},
	pages = {12,616--12,622},
	file = {Scher_2018_Toward_Data-Driven_Weather_and_Climate_Forecasting.pdf:/Users/tsmith/Drive/zotero/Scher_2018_Toward_Data-Driven_Weather_and_Climate_Forecasting.pdf:application/pdf;Snapshot:/Users/tsmith/Zotero/storage/DDXA5NIS/2018GL080704.html:text/html},
}


@article{hersbach_era5_2020,
	title = {The {ERA5} global reanalysis},
	volume = {146},
	issn = {1477-870X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/qj.3803},
	doi = {10.1002/qj.3803},
	abstract = {Within the Copernicus Climate Change Service (C3S), ECMWF is producing the ERA5 reanalysis which, once completed, will embody a detailed record of the global atmosphere, land surface and ocean waves from 1950 onwards. This new reanalysis replaces the ERA-Interim reanalysis (spanning 1979 onwards) which was started in 2006. ERA5 is based on the Integrated Forecasting System (IFS) Cy41r2 which was operational in 2016. ERA5 thus benefits from a decade of developments in model physics, core dynamics and data assimilation. In addition to a significantly enhanced horizontal resolution of 31 km, compared to 80 km for ERA-Interim, ERA5 has hourly output throughout, and an uncertainty estimate from an ensemble (3-hourly at half the horizontal resolution). This paper describes the general set-up of ERA5, as well as a basic evaluation of characteristics and performance, with a focus on the dataset from 1979 onwards which is currently publicly available. Re-forecasts from ERA5 analyses show a gain of up to one day in skill with respect to ERA-Interim. Comparison with radiosonde and PILOT data prior to assimilation shows an improved fit for temperature, wind and humidity in the troposphere, but not the stratosphere. A comparison with independent buoy data shows a much improved fit for ocean wave height. The uncertainty estimate reflects the evolution of the observing systems used in ERA5. The enhanced temporal and spatial resolution allows for a detailed evolution of weather systems. For precipitation, global-mean correlation with monthly-mean GPCP data is increased from 67\% to 77\%. In general, low-frequency variability is found to be well represented and from 10 hPa downwards general patterns of anomalies in temperature match those from the ERA-Interim, MERRA-2 and JRA-55 reanalyses.},
	language = {en},
	number = {730},
	journal = {Quarterly Journal of the Royal Meteorological Society},
	author = {Hersbach, Hans and Bell, Bill and Berrisford, Paul and Hirahara, Shoji and Horányi, András and Muñoz-Sabater, Joaquín and Nicolas, Julien and Peubey, Carole and Radu, Raluca and Schepers, Dinand and Simmons, Adrian and Soci, Cornel and Abdalla, Saleh and Abellan, Xavier and Balsamo, Gianpaolo and Bechtold, Peter and Biavati, Gionata and Bidlot, Jean and Bonavita, Massimo and De Chiara, Giovanna and Dahlgren, Per and Dee, Dick and Diamantakis, Michail and Dragani, Rossana and Flemming, Johannes and Forbes, Richard and Fuentes, Manuel and Geer, Alan and Haimberger, Leo and Healy, Sean and Hogan, Robin J. and Hólm, Elías and Janisková, Marta and Keeley, Sarah and Laloyaux, Patrick and Lopez, Philippe and Lupu, Cristina and Radnoti, Gabor and de Rosnay, Patricia and Rozum, Iryna and Vamborg, Freja and Villaume, Sebastien and Thépaut, Jean-Noël},
	year = {2020},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/qj.3803},
	keywords = {climate reanalysis, Copernicus Climate Change Service, data assimilation, ERA5, historical observations},
	pages = {1999--2049},
	file = {Hersbach_et_al_2020_The_ERA5_global_reanalysis.pdf:/Users/tsmith/Drive/zotero/Hersbach_et_al_2020_The_ERA5_global_reanalysis.pdf:application/pdf;Snapshot:/Users/tsmith/Zotero/storage/YRUI7UXB/qj.html:text/html},
}


@techreport{penny_coupled_2017,
	title = {Coupled {Data} {Assimilation} for {Integrated} {Earth} {System} {Analysis} and {Prediction}: {Goals}, {Challenges} and {Recommendations}},
	institution = {Geneva: World Meteorological Organization.},
	author = {Penny, Stephen G. and Akella, Santha and Alves, Oscar and Craig, Bishop and Buehner, Mark and Chevallier, Matthieu and Counillon, Francois and Draper, Clara and Frolov, Sergey and Fujii, Yosuke and Karspeck, Alicia and Kumar, Arun and Laloyaux, Patrick and Mahfouf, Jean-Francois and Martin, Matthew and Peña, Malaquias and de Rosnay, Patricia and Subramanian, Aneesh and Tardif, Robert and Wang, Yiguo and Wu, Xingren},
	year = {2017},
	file = {Penny_et_al_2017_Coupled_Data_Assimilation_for_Integrated_Earth_System_Analysis_and_Prediction.pdf:/Users/tsmith/Drive/zotero/Penny_et_al_2017_Coupled_Data_Assimilation_for_Integrated_Earth_System_Analysis_and_Prediction.pdf:application/pdf},
}


@article{hewitt_impact_2016,
	title = {The impact of resolving the {Rossby} radius at mid-latitudes in the ocean: results from a high-resolution version of the {Met} {Office} {GC2} coupled model},
	volume = {9},
	issn = {1991-959X},
	shorttitle = {The impact of resolving the {Rossby} radius at mid-latitudes in the ocean},
	url = {https://gmd.copernicus.org/articles/9/3655/2016/},
	doi = {10.5194/gmd-9-3655-2016},
	abstract = {{\textless}p{\textgreater}{\textless}strong class="journal-contentHeaderColor"{\textgreater}Abstract.{\textless}/strong{\textgreater} There is mounting evidence that resolving mesoscale eddies and western boundary currents as well as topographically controlled flows can play an important role in air–sea interaction associated with vertical and lateral transports of heat and salt. Here we describe the development of the Met Office Global Coupled Model version 2 (GC2) with increased resolution relative to the standard model: the ocean resolution is increased from 1/4 to 1/12° (28 to 9 km at the Equator), the atmosphere resolution increased from 60 km (N216) to 25 km (N512) and the coupling period reduced from 3 hourly to hourly. The technical developments that were required to build a version of the model at higher resolution are described as well as results from a 20-year simulation. The results demonstrate the key role played by the enhanced resolution of the ocean model: reduced sea surface temperature (SST) biases, improved ocean heat transports, deeper and stronger overturning circulation and a stronger Antarctic Circumpolar Current. Our results suggest that the improvements seen here require high resolution in both atmosphere and ocean components as well as high-frequency coupling. These results add to the body of evidence suggesting that ocean resolution is an important consideration when developing coupled models for weather and climate applications.{\textless}/p{\textgreater}},
	language = {English},
	number = {10},
	journal = {Geoscientific Model Development},
	author = {Hewitt, Helene T. and Roberts, Malcolm J. and Hyder, Pat and Graham, Tim and Rae, Jamie and Belcher, Stephen E. and Bourdallé-Badie, Romain and Copsey, Dan and Coward, Andrew and Guiavarch, Catherine and Harris, Chris and Hill, Richard and Hirschi, Joël J.-M. and Madec, Gurvan and Mizielinski, Matthew S. and Neininger, Erica and New, Adrian L. and Rioual, Jean-Christophe and Sinha, Bablu and Storkey, David and Shelly, Ann and Thorpe, Livia and Wood, Richard A.},
	month = oct,
	year = {2016},
	note = {Publisher: Copernicus GmbH},
	pages = {3655--3670},
	file = {Hewitt_et_al_2016_The_impact_of_resolving_the_Rossby_radius_at_mid-latitudes_in_the_ocean.pdf:/Users/tsmith/Drive/zotero/Hewitt_et_al_2016_The_impact_of_resolving_the_Rossby_radius_at_mid-latitudes_in_the_ocean.pdf:application/pdf;Snapshot:/Users/tsmith/Zotero/storage/679MIVS2/2016.html:text/html},
}

@book{evensen_data_2022,
	address = {Cham},
	series = {Springer {Textbooks} in {Earth} {Sciences}, {Geography} and {Environment}},
	title = {Data {Assimilation} {Fundamentals}: {A} {Unified} {Formulation} of the {State} and {Parameter} {Estimation} {Problem}},
	isbn = {978-3-030-96708-6 978-3-030-96709-3},
	shorttitle = {Data {Assimilation} {Fundamentals}},
	url = {https://link.springer.com/10.1007/978-3-030-96709-3},
	language = {en},
	publisher = {Springer International Publishing},
	author = {Evensen, Geir and Vossepoel, Femke C. and van Leeuwen, Peter Jan},
	year = {2022},
	doi = {10.1007/978-3-030-96709-3},
	file = {Evensen_et_al_2022_Data_Assimilation_Fundamentals.pdf:/Users/tsmith/Drive/zotero/Evensen_et_al_2022_Data_Assimilation_Fundamentals.pdf:application/pdf},
}


@article{bui-thanh_model_2008,
	title = {Model {Reduction} for {Large}-{Scale} {Systems} with {High}-{Dimensional} {Parametric} {Input} {Space}},
	volume = {30},
	issn = {1064-8275},
	url = {https://epubs.siam.org/doi/10.1137/070694855},
	doi = {10.1137/070694855},
	abstract = {A model-constrained adaptive sampling methodology is proposed for the reduction of large-scale systems with high-dimensional parametric input spaces. Our model reduction method uses a reduced basis approach, which requires the computation of high-fidelity solutions at a number of sample points throughout the parametric input space. A key challenge that must be addressed in the optimization, control, and probabilistic settings is the need for the reduced models to capture variation over this parametric input space, which, for many applications, will be of high dimension. We pose the task of determining appropriate sample points as a PDE-constrained optimization problem, which is implemented using an efficient adaptive algorithm that scales well to systems with a large number of parameters. The methodology is demonstrated using examples with parametric input spaces of dimension 11 and 21, which describe thermal analysis and design of a heat conduction fin, and compared with statistically based sampling methods. For these examples, the model-constrained adaptive sampling leads to reduced models that, for a given basis size, have error several orders of magnitude smaller than that obtained using the other methods.},
	number = {6},
	journal = {SIAM Journal on Scientific Computing},
	author = {Bui-Thanh, T. and Willcox, K. and Ghattas, O.},
	month = jan,
	year = {2008},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	keywords = {37M99, 37N40, 65K10, heat conduction, model reduction, optimization, sampling},
	pages = {3270--3288},
	file = {Bui-Thanh_et_al_2008_Model_Reduction_for_Large-Scale_Systems_with_High-Dimensional_Parametric_Input.pdf:/Users/tsmith/Drive/zotero/Bui-Thanh_et_al_2008_Model_Reduction_for_Large-Scale_Systems_with_High-Dimensional_Parametric_Input.pdf:application/pdf},
}

@article{najm_uncertainty_2009,
	title = {Uncertainty {Quantification} and {Polynomial} {Chaos} {Techniques} in {Computational} {Fluid} {Dynamics}},
	volume = {41},
	url = {https://doi.org/10.1146/annurev.fluid.010908.165248},
	doi = {10.1146/annurev.fluid.010908.165248},
	abstract = {The quantification of uncertainty in computational fluid dynamics (CFD) predictions is both a significant challenge and an important goal. Probabilistic uncertainty quantification (UQ) methods have been used to propagate uncertainty from model inputs to outputs when input uncertainties are large and have been characterized probabilistically. Polynomial chaos (PC) methods have found increased use in probabilistic UQ over the past decade. This review describes the use of PC expansions for the representation of random variables/fields and discusses their utility for the propagation of uncertainty in computational models, focusing on CFD models. Many CFD applications are considered, including flow in porous media, incompressible and compressible flows, and thermofluid and reacting flows. The review examines each application area, focusing on the demonstrated use of PC UQ and the associated challenges. Cross-cutting challenges with time unsteadiness and long time horizons are also discussed.},
	number = {1},
	journal = {Annual Review of Fluid Mechanics},
	author = {Najm, Habib N.},
	year = {2009},
	note = {\_eprint: https://doi.org/10.1146/annurev.fluid.010908.165248},
	keywords = {CFD, PC, polynomial chaos, UQ},
	pages = {35--52},
}


@article{huan_simulation-based_2013,
	title = {Simulation-based optimal {Bayesian} experimental design for nonlinear systems},
	volume = {232},
	issn = {0021-9991},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999112004597},
	doi = {10.1016/j.jcp.2012.08.013},
	abstract = {The optimal selection of experimental conditions is essential to maximizing the value of data for inference and prediction, particularly in situations where experiments are time-consuming and expensive to conduct. We propose a general mathematical framework and an algorithmic approach for optimal experimental design with nonlinear simulation-based models; in particular, we focus on finding sets of experiments that provide the most information about targeted sets of parameters. Our framework employs a Bayesian statistical setting, which provides a foundation for inference from noisy, indirect, and incomplete data, and a natural mechanism for incorporating heterogeneous sources of information. An objective function is constructed from information theoretic measures, reflecting expected information gain from proposed combinations of experiments. Polynomial chaos approximations and a two-stage Monte Carlo sampling method are used to evaluate the expected information gain. Stochastic approximation algorithms are then used to make optimization feasible in computationally intensive and high-dimensional settings. These algorithms are demonstrated on model problems and on nonlinear parameter inference problems arising in detailed combustion kinetics.},
	language = {en},
	number = {1},
	journal = {Journal of Computational Physics},
	author = {Huan, Xun and Marzouk, Youssef M.},
	month = jan,
	year = {2013},
	keywords = {Bayesian inference, Chemical kinetics, Nonlinear experimental design, Optimal experimental design, Shannon information, Stochastic approximation, Uncertainty quantification},
	pages = {288--317},
	file = {Huan_Marzouk_2013_Simulation-based_optimal_Bayesian_experimental_design_for_nonlinear_systems.pdf:/Users/tsmith/Drive/zotero/Huan_Marzouk_2013_Simulation-based_optimal_Bayesian_experimental_design_for_nonlinear_systems.pdf:application/pdf;ScienceDirect Snapshot:/Users/tsmith/Zotero/storage/5EWCJ8BV/S0021999112004597.html:text/html},
}


@article{hasselmann_pips_1988,
	title = {{PIPs} and {POPs}: {The} reduction of complex dynamical systems using principal interaction and oscillation patterns},
	volume = {93},
	issn = {2156-2202},
	shorttitle = {{PIPs} and {POPs}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1029/JD093iD09p11015},
	doi = {10.1029/JD093iD09p11015},
	abstract = {A general method is described for constructing simple dynamical models to approximate complex dynamical systems with many degrees of freedom. The technique can be applied to interpret sets of observed time series or numerical simulations with high-resolution models, or to relate observation and simulations. The method is based on a projection of the complete system on to a smaller number of “principal interaction patterns” (PIPs). The coefficients of the PIP expansion are assumed to be governed by a dynamic model containing a small number of adjustable parameters. The optimization of the dynamical model, which in the general case can be both nonlinear and time-dependent, is carried out simultaneously with the construction of the optimal set of interaction patterns. In the linear case the PIPs reduce to the eigenoscilations of a first-order linear vector process with stochastic forcing (principal oscillation patterns, or POPs). POPs are linearly related to the “principal prediction patterns” used in linear forecasting applications. The POP analysis can also be applied as a diagnostic tool to compress the extensive information contained in the high-dimensional cross-spectral covariance matrix representing the complete second-moment structure of the system.},
	language = {en},
	number = {D9},
	journal = {Journal of Geophysical Research: Atmospheres},
	author = {Hasselmann, K.},
	year = {1988},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1029/JD093iD09p11015},
	pages = {11015--11021},
	file = {Hasselmann_1988_PIPs_and_POPs.pdf:/Users/tsmith/Drive/zotero/Hasselmann_1988_PIPs_and_POPs.pdf:application/pdf;Snapshot:/Users/tsmith/Zotero/storage/LPT6T3AZ/JD093iD09p11015.html:text/html},
}


@article{cressie_statistics_1993,
  title={Statistics for spatial data},
  author={Cressie, N},
  year={1993},
  publisher={Wiley Location New York, NY}
}
@article{penland_random_1989,
	title = {Random {Forcing} and {Forecasting} {Using} {Principal} {Oscillation} {Pattern} {Analysis}},
	volume = {117},
	issn = {1520-0493, 0027-0644},
	url = {https://journals.ametsoc.org/view/journals/mwre/117/10/1520-0493_1989_117_2165_rfafup_2_0_co_2.xml},
	doi = {10.1175/1520-0493(1989)117<2165:RFAFUP>2.0.CO;2},
	abstract = {Abstract The effects of random forcing and deterministic feedback are combined in a measured multivariate time series. It is shown here how the characteristics of the driving noise can be found after the deterministic effects have been identified by the principal oscillation pattern (POP) analysis. In addition, the POP analysis is extended to enable the prediction of the most probable meteorological pattern at some future time when the present pattern is known, and the conditional probability of finding the process at any location within a range of values given the value of the process at another location at an earlier time. Estimates of how well these predictions can be trusted are also given. The basic assumption of POP analysis is that the system can be optimally modeled by a linear Markov process.},
	language = {EN},
	number = {10},
	journal = {Monthly Weather Review},
	author = {Penland, Cecile},
	month = oct,
	year = {1989},
	note = {Publisher: American Meteorological Society
Section: Monthly Weather Review},
	pages = {2165--2185},
	file = {Penland_1989_Random_Forcing_and_Forecasting_Using_Principal_Oscillation_Pattern_Analysis.pdf:/Users/tsmith/Drive/zotero/Penland_1989_Random_Forcing_and_Forecasting_Using_Principal_Oscillation_Pattern_Analysis.pdf:application/pdf;Snapshot:/Users/tsmith/Zotero/storage/VCQU392N/1520-0493_1989_117_2165_rfafup_2_0_co_2.html:text/html},
}

@incollection{goodfellow_sequence_2016,
	title = {Sequence {Modeling}: {Recurrent} and {Recursive} {Nets}},
	url = {https://www.deeplearningbook.org/},
	booktitle = {Deep {Learning}},
	publisher = {MIT Press},
	author = {Goodfellow, Ian and Yoshua, Bengio and Aaron, Courville},
	year = {2016},
	file = {Goodfellow_et_al_2016_Sequence_Modeling.pdf:/Users/tsmith/Drive/zotero/Goodfellow_et_al_2016_Sequence_Modeling.pdf:application/pdf},
}

@article{platt_systematic_2022,
	title = {A systematic exploration of reservoir computing for forecasting complex spatiotemporal dynamics},
	volume = {153},
	copyright = {All rights reserved},
	issn = {0893-6080},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608022002404},
	doi = {10.1016/j.neunet.2022.06.025},
	abstract = {A reservoir computer (RC) is a type of recurrent neural network architecture with demonstrated success in the prediction of spatiotemporally chaotic dynamical systems. A further advantage of RC is that it reproduces intrinsic dynamical quantities essential for its incorporation into numerical forecasting routines such as the ensemble Kalman filter—used in numerical weather prediction to compensate for sparse and noisy data. We explore here the architecture and design choices for a “best in class” RC for a number of characteristic dynamical systems. Our analysis points to the importance of large scale parameter optimization. We also note in particular the importance of including input bias in the RC design, which has a significant impact on the forecast skill of the trained RC model. In our tests, the use of a nonlinear readout operator does not affect the forecast time or the stability of the forecast. The effects of the reservoir dimension, spinup time, amount of training data, normalization, noise, and the RC time step are also investigated. Finally, we detail how our investigation leads to optimal design choices for a parallel RC scheme applied to the 40 dimensional spatiotemporally chaotic Lorenz 1996 dynamics.},
	language = {en},
	journal = {Neural Networks},
	author = {Platt, Jason A. and Penny, Stephen G. and Smith, Timothy A. and Chen, Tse-Chun and Abarbanel, Henry D. I.},
	month = sep,
	year = {2022},
	keywords = {Nonlinear dynamical systems, Machine learning, Reservoir computing, Chaotic time series forecasting, Echo-state networks, Recurrent neural network},
	pages = {530--552},
	file = {Platt_et_al_2022_A_systematic_exploration_of_reservoir_computing_for_forecasting_complex.pdf:/Users/tsmith/Drive/zotero/Platt_et_al_2022_A_systematic_exploration_of_reservoir_computing_for_forecasting_complex.pdf:application/pdf;ScienceDirect Snapshot:/Users/tsmith/Zotero/storage/R3CER5MW/S0893608022002404.html:text/html},
}


@article{pathak_model-free_2018,
	title = {Model-{Free} {Prediction} of {Large} {Spatiotemporally} {Chaotic} {Systems} from {Data}: {A} {Reservoir} {Computing} {Approach}},
	volume = {120},
	shorttitle = {Model-{Free} {Prediction} of {Large} {Spatiotemporally} {Chaotic} {Systems} from {Data}},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.120.024102},
	doi = {10.1103/PhysRevLett.120.024102},
	abstract = {We demonstrate the effectiveness of using machine learning for model-free prediction of spatiotemporally chaotic systems of arbitrarily large spatial extent and attractor dimension purely from observations of the system’s past evolution. We present a parallel scheme with an example implementation based on the reservoir computing paradigm and demonstrate the scalability of our scheme using the Kuramoto-Sivashinsky equation as an example of a spatiotemporally chaotic system.},
	number = {2},
	journal = {Physical Review Letters},
	author = {Pathak, Jaideep and Hunt, Brian and Girvan, Michelle and Lu, Zhixin and Ott, Edward},
	month = jan,
	year = {2018},
	note = {Publisher: American Physical Society},
	pages = {024102},
	file = {APS Snapshot:/Users/tsmith/Zotero/storage/KXZGCJ3B/PhysRevLett.120.html:text/html;Pathak_et_al_2018_Model-Free_Prediction_of_Large_Spatiotemporally_Chaotic_Systems_from_Data.pdf:/Users/tsmith/Drive/zotero/Pathak_et_al_2018_Model-Free_Prediction_of_Large_Spatiotemporally_Chaotic_Systems_from_Data.pdf:application/pdf;Pathak_et_al_2018_Model-Free_Prediction_of_Large_Spatiotemporally_Chaotic_Systems_from_Data.pdf:/Users/tsmith/Drive/zotero/Pathak_et_al_2018_Model-Free_Prediction_of_Large_Spatiotemporally_Chaotic_Systems_from_Data2.pdf:application/pdf},
}

@article{lu_attractor_2018,
	title = {Attractor reconstruction by machine learning},
	volume = {28},
	issn = {1054-1500},
	url = {http://aip.scitation.org/doi/10.1063/1.5039508},
	doi = {10.1063/1.5039508},
	abstract = {A machine-learning approach called “reservoir computing” has been used successfully for short-term prediction and attractor reconstruction of chaotic dynamical systems from time series data. We present a theoretical framework that describes conditions under which reservoir computing can create an empirical model capable of skillful short-term forecasts and accurate long-term ergodic behavior. We illustrate this theory through numerical experiments. We also argue that the theory applies to certain other machine learning methods for time series prediction.},
	number = {6},
	journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
	author = {Lu, Zhixin and Hunt, Brian R. and Ott, Edward},
	month = jun,
	year = {2018},
	note = {Publisher: American Institute of Physics},
	pages = {061104},
	file = {Lu_et_al_2018_Attractor_reconstruction_by_machine_learning.pdf:/Users/tsmith/Drive/zotero/Lu_et_al_2018_Attractor_reconstruction_by_machine_learning.pdf:application/pdf;Snapshot:/Users/tsmith/Zotero/storage/N3T3F3WN/1.html:text/html},
}

@article{lu_reservoir_2017,
	title = {Reservoir observers: {Model}-free inference of unmeasured variables in chaotic systems},
	volume = {27},
	issn = {1054-1500},
	shorttitle = {Reservoir observers},
	url = {https://aip-scitation-org.colorado.idm.oclc.org/doi/10.1063/1.4979665},
	doi = {10.1063/1.4979665},
	abstract = {Deducing the state of a dynamical system as a function of time from a limited number of concurrent system state measurements is an important problem of great practical utility. A scheme that accomplishes this is called an “observer.” We consider the case in which a model of the system is unavailable or insufficiently accurate, but “training” time series data of the desired state variables are available for a short period of time, and a limited number of other system variables are continually measured. We propose a solution to this problem using networks of neuron-like units known as “reservoir computers.” The measurements that are continually available are input to the network, which is trained with the limited-time data to output estimates of the desired state variables. We demonstrate our method, which we call a “reservoir observer,” using the Rössler system, the Lorenz system, and the spatiotemporally chaotic Kuramoto–Sivashinsky equation. Subject to the condition of observability (i.e., whether it is in principle possible, by any means, to infer the desired unmeasured variables from the measured variables), we show that the reservoir observer can be a very effective and versatile tool for robustly reconstructing unmeasured dynamical system variables.},
	number = {4},
	journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
	author = {Lu, Zhixin and Pathak, Jaideep and Hunt, Brian and Girvan, Michelle and Brockett, Roger and Ott, Edward},
	month = apr,
	year = {2017},
	note = {Publisher: American Institute of Physics},
	pages = {041102},
	file = {Lu_et_al_2017_Reservoir_observers.pdf:/Users/tsmith/Drive/zotero/Lu_et_al_2017_Reservoir_observers.pdf:application/pdf;Snapshot:/Users/tsmith/Zotero/storage/EN8VSP3V/1.html:text/html},
}

@article{gauthier_next_2021,
	title = {Next generation reservoir computing},
	volume = {12},
	copyright = {2021 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-021-25801-2},
	doi = {10.1038/s41467-021-25801-2},
	abstract = {Reservoir computing is a best-in-class machine learning algorithm for processing information generated by dynamical systems using observed time-series data. Importantly, it requires very small training data sets, uses linear optimization, and thus requires minimal computing resources. However, the algorithm uses randomly sampled matrices to define the underlying recurrent neural network and has a multitude of metaparameters that must be optimized. Recent results demonstrate the equivalence of reservoir computing to nonlinear vector autoregression, which requires no random matrices, fewer metaparameters, and provides interpretable results. Here, we demonstrate that nonlinear vector autoregression excels at reservoir computing benchmark tasks and requires even shorter training data sets and training time, heralding the next generation of reservoir computing.},
	language = {en},
	number = {1},
	journal = {Nature Communications},
	author = {Gauthier, Daniel J. and Bollt, Erik and Griffith, Aaron and Barbosa, Wendson A. S.},
	month = sep,
	year = {2021},
	note = {Bandiera\_abtest: a
Cc\_license\_type: cc\_by
Cg\_type: Nature Research Journals
Number: 1
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Computational science;Electrical and electronic engineering
Subject\_term\_id: computational-science;electrical-and-electronic-engineering},
	pages = {5564},
	file = {Gauthier_et_al_2021_Next_generation_reservoir_computing.pdf:/Users/tsmith/Drive/zotero/Gauthier_et_al_2021_Next_generation_reservoir_computing.pdf:application/pdf;Gauthier_et_al_2021_Next_generation_reservoir_computing.pdf:/Users/tsmith/Drive/zotero/Gauthier_et_al_2021_Next_generation_reservoir_computing2.pdf:application/pdf;Snapshot:/Users/tsmith/Zotero/storage/JTK6AW8H/s41467-021-25801-2.html:text/html},
}

@article{bollt_explaining_2021,
	title = {On explaining the surprising success of reservoir computing forecaster of chaos? {The} universal machine learning dynamical system with contrast to {VAR} and {DMD}},
	volume = {31},
	issn = {1054-1500},
	shorttitle = {On explaining the surprising success of reservoir computing forecaster of chaos?},
	url = {https://aip-scitation-org.colorado.idm.oclc.org/doi/10.1063/5.0024890},
	doi = {10.1063/5.0024890},
	abstract = {Machine learning has become a widely popular and successful paradigm, especially in data-driven science and engineering. A major application problem is data-driven forecasting of future states from a complex dynamical system. Artificial neural networks have evolved as a clear leader among many machine learning approaches, and recurrent neural networks are considered to be particularly well suited for forecasting dynamical systems. In this setting, the echo-state networks or reservoir computers (RCs) have emerged for their simplicity and computational complexity advantages. Instead of a fully trained network, an RC trains only readout weights by a simple, efficient least squares method. What is perhaps quite surprising is that nonetheless, an RC succeeds in making high quality forecasts, competitively with more intensively trained methods, even if not the leader. There remains an unanswered question as to why and how an RC works at all despite randomly selected weights. To this end, this work analyzes a further simplified RC, where the internal activation function is an identity function. Our simplification is not presented for the sake of tuning or improving an RC, but rather for the sake of analysis of what we take to be the surprise being not that it does not work better, but that such random methods work at all. We explicitly connect the RC with linear activation and linear readout to well developed time-series literature on vector autoregressive (VAR) averages that includes theorems on representability through the Wold theorem, which already performs reasonably for short-term forecasts. In the case of a linear activation and now popular quadratic readout RC, we explicitly connect to a nonlinear VAR, which performs quite well. Furthermore, we associate this paradigm to the now widely popular dynamic mode decomposition; thus, these three are in a sense different faces of the same thing. We illustrate our observations in terms of popular benchmark examples including Mackey–Glass differential delay equations and the Lorenz63 system.},
	number = {1},
	journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
	author = {Bollt, Erik},
	month = jan,
	year = {2021},
	note = {Publisher: American Institute of Physics},
	pages = {013108},
	file = {Bollt_2021_On_explaining_the_surprising_success_of_reservoir_computing_forecaster_of_chaos.pdf:/Users/tsmith/Drive/zotero/Bollt_2021_On_explaining_the_surprising_success_of_reservoir_computing_forecaster_of_chaos.pdf:application/pdf;Bollt_2021_On_explaining_the_surprising_success_of_reservoir_computing_forecaster_of_chaos.pdf:/Users/tsmith/Drive/zotero/Bollt_2021_On_explaining_the_surprising_success_of_reservoir_computing_forecaster_of_chaos2.pdf:application/pdf},
}

@article{griffith_forecasting_2019,
	title = {Forecasting chaotic systems with very low connectivity reservoir computers},
	volume = {29},
	issn = {1054-1500},
	url = {https://aip.scitation.org/doi/10.1063/1.5120710},
	doi = {10.1063/1.5120710},
	number = {12},
	journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
	author = {Griffith, Aaron and Pomerance, Andrew and Gauthier, Daniel J.},
	month = dec,
	year = {2019},
	note = {Publisher: American Institute of Physics},
	pages = {123108},
	file = {Griffith_et_al_2019_Forecasting_chaotic_systems_with_very_low_connectivity_reservoir_computers.pdf:/Users/tsmith/Drive/zotero/Griffith_et_al_2019_Forecasting_chaotic_systems_with_very_low_connectivity_reservoir_computers.pdf:application/pdf},
}

@article{verstraeten_experimental_2007,
	title = {An experimental unification of reservoir computing methods},
	volume = {20},
	issn = {08936080},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S089360800700038X},
	doi = {10.1016/j.neunet.2007.04.003},
	abstract = {Three different uses of a recurrent neural network (RNN) as a reservoir that is not trained but instead read out by a simple external classiﬁcation layer have been described in the literature: Liquid State Machines (LSMs), Echo State Networks (ESNs) and the Backpropagation Decorrelation (BPDC) learning rule. Individual descriptions of these techniques exist, but a overview is still lacking. Here, we present a series of experimental results that compares all three implementations, and draw conclusions about the relation between a broad range of reservoir parameters and network dynamics, memory, node complexity and performance on a variety of benchmark tests with different characteristics. Next, we introduce a new measure for the reservoir dynamics based on Lyapunov exponents. Unlike previous measures in the literature, this measure is dependent on the dynamics of the reservoir in response to the inputs, and in the cases we tried, it indicates an optimal value for the global scaling of the weight matrix, irrespective of the standard measures. We also describe the Reservoir Computing Toolbox that was used for these experiments, which implements all the types of Reservoir Computing and allows the easy simulation of a wide range of reservoir topologies for a number of benchmarks.},
	language = {en},
	number = {3},
	journal = {Neural Networks},
	author = {Verstraeten, D. and Schrauwen, B. and D’Haene, M. and Stroobandt, D.},
	month = apr,
	year = {2007},
	pages = {391--403},
	file = {Verstraeten_et_al_2007_An_experimental_unification_of_reservoir_computing_methods.pdf:/Users/tsmith/Drive/zotero/Verstraeten_et_al_2007_An_experimental_unification_of_reservoir_computing_methods.pdf:application/pdf},
}

@article{yildiz_re-visiting_2012,
	title = {Re-visiting the echo state property},
	volume = {35},
	issn = {0893-6080},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608012001852},
	doi = {10.1016/j.neunet.2012.07.005},
	abstract = {An echo state network (ESN) consists of a large, randomly connected neural network, the reservoir, which is driven by an input signal and projects to output units. During training, only the connections from the reservoir to these output units are learned. A key requisite for output-only training is the echo state property (ESP), which means that the effect of initial conditions should vanish as time passes. In this paper, we use analytical examples to show that a widely used criterion for the ESP, the spectral radius of the weight matrix being smaller than unity, is not sufficient to satisfy the echo state property. We obtain these examples by investigating local bifurcation properties of the standard ESNs. Moreover, we provide new sufficient conditions for the echo state property of standard sigmoid and leaky integrator ESNs. We furthermore suggest an improved technical definition of the echo state property, and discuss what practicians should (and should not) observe when they optimize their reservoirs for specific tasks.},
	language = {en},
	journal = {Neural Networks},
	author = {Yildiz, Izzet B. and Jaeger, Herbert and Kiebel, Stefan J.},
	month = nov,
	year = {2012},
	keywords = {Echo state network, Bifurcation, Diagonally Schur stable, Lyapunov, Spectral radius},
	pages = {1--9},
	file = {ScienceDirect Snapshot:/Users/tsmith/Zotero/storage/L9JZHSC3/S0893608012001852.html:text/html;Yildiz_et_al_2012_Re-visiting_the_echo_state_property.pdf:/Users/tsmith/Drive/zotero/Yildiz_et_al_2012_Re-visiting_the_echo_state_property.pdf:application/pdf},
}

@article{lymburn_reservoirs_2019,
	title = {The reservoir’s perspective on generalized synchronization},
	volume = {29},
	issn = {1054-1500},
	url = {https://aip-scitation-org.colorado.idm.oclc.org/doi/10.1063/1.5120733},
	doi = {10.1063/1.5120733},
	abstract = {We employ reservoir computing for a reconstruction task in coupled chaotic systems, across a range of dynamical relationships including generalized synchronization. For a drive-response setup, a temporal representation of the synchronized state is discussed as an alternative to the known instantaneous form. The reservoir has access to both representations through its fading memory property, each with advantages in different dynamical regimes. We also extract signatures of the maximal conditional Lyapunov exponent in the performance of variations of the reservoir topology. Moreover, the reservoir model reproduces different levels of consistency where there is no synchronization. In a bidirectional coupling setup, high reconstruction accuracy is achieved despite poor observability and independent of generalized synchronization.},
	number = {9},
	journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
	author = {Lymburn, Thomas and Walker, David M. and Small, Michael and Jüngling, Thomas},
	month = sep,
	year = {2019},
	note = {Publisher: American Institute of Physics},
	pages = {093133},
	file = {Lymburn_et_al_2019_The_reservoir’s_perspective_on_generalized_synchronization.pdf:/Users/tsmith/Drive/zotero/Lymburn_et_al_2019_The_reservoir’s_perspective_on_generalized_synchronization.pdf:application/pdf},
}

@article{platt_robust_2021,
	title = {Robust forecasting using predictive generalized synchronization in reservoir computing},
	volume = {31},
	issn = {1054-1500},
	url = {https://aip-scitation-org.colorado.idm.oclc.org/doi/10.1063/5.0066013},
	doi = {10.1063/5.0066013},
	abstract = {Reservoir computers (RCs) are a class of recurrent neural networks (RNNs) that can be used for forecasting the future of observed time series data. As with all RNNs, selecting the hyperparameters in the network to yield excellent forecasting presents a challenge when training on new inputs. We analyze a method based on predictive generalized synchronization (PGS) that gives direction in designing and evaluating the architecture and hyperparameters of an RC. To determine the occurrences of PGS, we rely on the auxiliary method to provide a computationally efficient pre-training test that guides hyperparameter selection. We provide a metric for evaluating the RC using the reproduction of the input system’s Lyapunov exponents that demonstrates robustness in prediction.},
	number = {12},
	journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
	author = {Platt, Jason A. and Wong, Adrian and Clark, Randall and Penny, Stephen G. and Abarbanel, Henry D. I.},
	month = dec,
	year = {2021},
	note = {Publisher: American Institute of Physics},
	pages = {123118},
	file = {Platt_et_al_2021_Robust_forecasting_using_predictive_generalized_synchronization_in_reservoir.pdf:/Users/tsmith/Drive/zotero/Platt_et_al_2021_Robust_forecasting_using_predictive_generalized_synchronization_in_reservoir.pdf:application/pdf;Platt_et_al_2021_Robust_forecasting_using_predictive_generalized_synchronization_in_reservoir.pdf:/Users/tsmith/Drive/zotero/Platt_et_al_2021_Robust_forecasting_using_predictive_generalized_synchronization_in_reservoir2.pdf:application/pdf},
}

@article{weng_synchronization_2019,
	title = {Synchronization of chaotic systems and their machine-learning models},
	volume = {99},
	url = {https://link.aps.org/doi/10.1103/PhysRevE.99.042203},
	doi = {10.1103/PhysRevE.99.042203},
	abstract = {Recent advances have demonstrated the effectiveness of a machine-learning approach known as “reservoir computing” for model-free prediction of chaotic systems. We find that a well-trained reservoir computer can synchronize with its learned chaotic systems by linking them with a common signal. A necessary condition for achieving this synchronization is the negative values of the sub-Lyapunov exponents. Remarkably, we show that by sending just a scalar signal, one can achieve synchronism in trained reservoir computers and a cascading synchronization among chaotic systems and their fitted reservoir computers. Moreover, we demonstrate that this synchronization is maintained even in the presence of a parameter mismatch. Our findings possibly provide a path for accurate production of all expected signals in unknown chaotic systems using just one observational measure.},
	number = {4},
	journal = {Physical Review E},
	author = {Weng, Tongfeng and Yang, Huijie and Gu, Changgui and Zhang, Jie and Small, Michael},
	month = apr,
	year = {2019},
	note = {Publisher: American Physical Society},
	pages = {042203},
	file = {APS Snapshot:/Users/tsmith/Zotero/storage/T3WDWBDE/PhysRevE.99.html:text/html;Weng_et_al_2019_Synchronization_of_chaotic_systems_and_their_machine-learning_models.pdf:/Users/tsmith/Drive/zotero/Weng_et_al_2019_Synchronization_of_chaotic_systems_and_their_machine-learning_models.pdf:application/pdf},
}

@article{jaeger_echo_2001,
	title = {The "echo state” approach to analysing and training recurrent neural networks – with an {Erratum} note},
	volume = {148},
	language = {en},
	number = {34},
	journal = {Bonn, Germany: German National Research Center for Information Technology GMD Technical Report},
	author = {Jaeger, Herbert},
	year = {2001},
	pages = {13},
	file = {Jaeger_The_“echo_state”_approach_to_analysing_and_training_recurrent_neural_networks_–.pdf:/Users/tsmith/Drive/zotero/Jaeger_The_“echo_state”_approach_to_analysing_and_training_recurrent_neural_networks_–.pdf:application/pdf},
}

@article{barbosa_learning_2022,
	title = {Learning {Spatiotemporal} {Chaos} {Using} {Next}-{Generation} {Reservoir} {Computing}},
	url = {http://arxiv.org/abs/2203.13294},
	abstract = {Forecasting the behavior of high-dimensional dynamical systems using machine learning (ML) requires efficient methods to learn the underlying physical model. We demonstrate spatiotemporal chaos prediction of a heuristic atmospheric weather model using an ML architecture that, when combined with a next-generation reservoir computer, displays state-of-the-art performance with a training time \$10{\textasciicircum}3-10{\textasciicircum}4\$ times faster and training data set \${\textbackslash}sim 10{\textasciicircum}2\$ times smaller than other ML algorithms. We also take advantage of the translational symmetry of the model to further reduce the computational cost and training data, each by a factor of \${\textbackslash}sim\$10.},
	journal = {arXiv:2203.13294 [nlin]},
	author = {Barbosa, Wendson A. S. and Gauthier, Daniel J.},
	month = mar,
	year = {2022},
	note = {arXiv: 2203.13294},
	keywords = {Computer Science - Machine Learning, Nonlinear Sciences - Chaotic Dynamics, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: 5 pages, 4 figures},
	file = {arXiv.org Snapshot:/Users/tsmith/Zotero/storage/C4LAP874/2203.html:text/html;Barbosa_Gauthier_2022_Learning_Spatiotemporal_Chaos_Using_Next-Generation_Reservoir_Computing.pdf:/Users/tsmith/Drive/zotero/Barbosa_Gauthier_2022_Learning_Spatiotemporal_Chaos_Using_Next-Generation_Reservoir_Computing.pdf:application/pdf},
}


@article{vlachas_backpropagation_2020,
	title = {Backpropagation algorithms and {Reservoir} {Computing} in {Recurrent} {Neural} {Networks} for the forecasting of complex spatiotemporal dynamics},
	volume = {126},
	issn = {0893-6080},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608020300708},
	doi = {10.1016/j.neunet.2020.02.016},
	abstract = {We examine the efficiency of Recurrent Neural Networks in forecasting the spatiotemporal dynamics of high dimensional and reduced order complex systems using Reservoir Computing (RC) and Backpropagation through time (BPTT) for gated network architectures. We highlight advantages and limitations of each method and discuss their implementation for parallel computing architectures. We quantify the relative prediction accuracy of these algorithms for the long-term forecasting of chaotic systems using as benchmarks the Lorenz-96 and the Kuramoto–Sivashinsky (KS) equations. We find that, when the full state dynamics are available for training, RC outperforms BPTT approaches in terms of predictive performance and in capturing of the long-term statistics, while at the same time requiring much less training time. However, in the case of reduced order data, large scale RC models can be unstable and more likely than the BPTT algorithms to diverge. In contrast, RNNs trained via BPTT show superior forecasting abilities and capture well the dynamics of reduced order systems. Furthermore, the present study quantifies for the first time the Lyapunov Spectrum of the KS equation with BPTT, achieving similar accuracy as RC. This study establishes that RNNs are a potent computational framework for the learning and forecasting of complex spatiotemporal systems.},
	language = {en},
	journal = {Neural Networks},
	author = {Vlachas, P. R. and Pathak, J. and Hunt, B. R. and Sapsis, T. P. and Girvan, M. and Ott, E. and Koumoutsakos, P.},
	month = jun,
	year = {2020},
	keywords = {Reservoir Computing, Complex systems, Kuramoto–Sivashinsky, Lorenz-96, RNN, LSTM, GRU, Time series forecasting},
	pages = {191--217},
	file = {ScienceDirect Snapshot:/Users/tsmith/Zotero/storage/CJTELVEP/S0893608020300708.html:text/html;Vlachas_et_al_2020_Backpropagation_algorithms_and_Reservoir_Computing_in_Recurrent_Neural_Networks.pdf:/Users/tsmith/Drive/zotero/Vlachas_et_al_2020_Backpropagation_algorithms_and_Reservoir_Computing_in_Recurrent_Neural_Networks.pdf:application/pdf},
}


@article{penny_integrating_2022,
	title = {Integrating {Recurrent} {Neural} {Networks} {With} {Data} {Assimilation} for {Scalable} {Data}-{Driven} {State} {Estimation}},
	volume = {14},
	issn = {1942-2466},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1029/2021MS002843},
	doi = {10.1029/2021MS002843},
	abstract = {Data assimilation (DA) is integrated with machine learning in order to perform entirely data-driven online state estimation. To achieve this, recurrent neural networks (RNNs) are implemented as pretrained surrogate models to replace key components of the DA cycle in numerical weather prediction (NWP), including the conventional numerical forecast model, the forecast error covariance matrix, and the tangent linear and adjoint models. It is shown how these RNNs can be initialized using DA methods to directly update the hidden/reservoir state with observations of the target system. The results indicate that these techniques can be applied to estimate the state of a system for the repeated initialization of short-term forecasts, even in the absence of a traditional numerical forecast model. Further, it is demonstrated how these integrated RNN-DA methods can scale to higher dimensions by applying domain localization and parallelization, providing a path for practical applications in NWP.},
	language = {en},
	number = {3},
	journal = {Journal of Advances in Modeling Earth Systems},
	author = {Penny, S. G. and Smith, T. A. and Chen, T.-C. and Platt, J. A. and Lin, H.-Y. and Goodliff, M. and Abarbanel, H. D. I.},
	year = {2022},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1029/2021MS002843},
	keywords = {machine learning, data assimilation, artificial intelligence, recurrent neural networks, 4D-var, ensemble kalman filter},
	pages = {e2021MS002843},
	annote = {e2021MS002843 2021MS002843},
	file = {Penny_et_al_2022_Integrating_Recurrent_Neural_Networks_With_Data_Assimilation_for_Scalable.pdf:/Users/tsmith/Drive/zotero/Penny_et_al_2022_Integrating_Recurrent_Neural_Networks_With_Data_Assimilation_for_Scalable.pdf:application/pdf;Penny_et_al_2022_Integrating_Recurrent_Neural_Networks_With_Data_Assimilation_for_Scalable.pdf:/Users/tsmith/Drive/zotero/Penny_et_al_2022_Integrating_Recurrent_Neural_Networks_With_Data_Assimilation_for_Scalable.pdf:application/pdf;Snapshot:/Users/tsmith/Zotero/storage/E5CGG58J/2021MS002843.html:text/html},
}

@ARTICLE{scipy_2020,
  author  = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and
            Haberland, Matt and Reddy, Tyler and Cournapeau, David and
            Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and
            Bright, Jonathan and {van der Walt}, St{\'e}fan J. and
            Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and
            Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and
            Kern, Robert and Larson, Eric and Carey, C J and
            Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and
            {VanderPlas}, Jake and Laxalde, Denis and Perktold, Josef and
            Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and
            Harris, Charles R. and Archibald, Anne M. and
            Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and
            {van Mulbregt}, Paul and {SciPy 1.0 Contributors}},
  title   = {{{SciPy} 1.0: Fundamental Algorithms for Scientific
            Computing in Python}},
  journal = {Nature Methods},
  year    = {2020},
  volume  = {17},
  pages   = {261--272},
  adsurl  = {https://rdcu.be/b08Wh},
  doi     = {10.1038/s41592-019-0686-2},
}

@article{tikhonov_solution_1963,
  author = {Tikhonov, A. N.},
  journal = {Soviet Math. Dokl},
  keywords = {imported},
  title = {Solution of Incorrectly Formulated Problems
                  and the Regularization Method},
  year = 1963
}


@article{pathak_using_2017,
	title = {Using machine learning to replicate chaotic attractors and calculate {Lyapunov} exponents from data},
	volume = {27},
	issn = {1054-1500},
	url = {https://aip.scitation.org/doi/10.1063/1.5010300},
	doi = {10.1063/1.5010300},
	number = {12},
	journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
	author = {Pathak, Jaideep and Lu, Zhixin and Hunt, Brian R. and Girvan, Michelle and Ott, Edward},
	month = dec,
	year = {2017},
	note = {Publisher: American Institute of Physics},
	pages = {121102},
	file = {Pathak_et_al_2017_Using_machine_learning_to_replicate_chaotic_attractors_and_calculate_Lyapunov.pdf:/Users/tsmith/Drive/zotero/Pathak_et_al_2017_Using_machine_learning_to_replicate_chaotic_attractors_and_calculate_Lyapunov.pdf:application/pdf},
}

@article{maass_real-time_2002,
	title = {Real-{Time} {Computing} {Without} {Stable} {States}: {A} {New} {Framework} for {Neural} {Computation} {Based} on {Perturbations}},
	volume = {14},
	issn = {0899-7667, 1530-888X},
	shorttitle = {Real-{Time} {Computing} {Without} {Stable} {States}},
	url = {https://direct.mit.edu/neco/article/14/11/2531-2560/6650},
	doi = {10.1162/089976602760407955},
	abstract = {A key challenge for neural modeling is to explain how a continuous stream of multimodal input from a rapidly changing environment can be processed by stereotypical recurrent circuits of integrate-and-fire neurons in real time. We propose a new computational model for real-time computing on time-varying input that provides an alternative to paradigms based on Turing machines or attractor neural networks. It does not require a task-dependent construction of neural circuits. Instead, it is based on principles of high-dimensional dynamical systems in combination with statistical learning theory and can be implemented on generic evolved or found recurrent circuitry. It is shown that the inherent transient dynamics of the high-dimensional dynamical system formed by a sufficiently large and heterogeneous neural circuit may serve as universal analog fading memory. Readout neurons can learn to extract in real time from the current state of such recurrent neural circuit information about current and past inputs that may be needed for diverse tasks. Stable internal states are not required for giving a stable output, since transient internal states can be transformed by readout neurons into stable target outputs due to the high dimensionality of the dynamical system. Our approach is based on a rigorous computational model, the liquid state machine, that, unlike Turing machines, does not require sequential transitions between well-defined discrete internal states. It is supported, as the Turing machine is, by rigorous mathematical results that predict universal computational power under idealized conditions, but for the biologically more realistic scenario of real-time processing of time-varying inputs. Our approach provides new perspectives for the interpretation of neural coding, the design of experiments and data analysis in neurophysiology, and the solution of problems in robotics and neurotechnology.},
	language = {en},
	number = {11},
	journal = {Neural Computation},
	author = {Maass, Wolfgang and Natschläger, Thomas and Markram, Henry},
	month = nov,
	year = {2002},
	pages = {2531--2560},
	file = {Maass_et_al_2002_Real-Time_Computing_Without_Stable_States.pdf:/Users/tsmith/Drive/zotero/Maass_et_al_2002_Real-Time_Computing_Without_Stable_States2.pdf:application/pdf},
}

@inproceedings{steil_backpropagation-decorrelation_2004,
	title = {Backpropagation-decorrelation: online recurrent learning with {O}({N}) complexity},
	volume = {2},
	shorttitle = {Backpropagation-decorrelation},
	doi = {10.1109/IJCNN.2004.1380039},
	abstract = {We introduce a new learning rule for fully recurrent neural networks which we call backpropagation-decorrelation rule (BPDC). It combines important principles: one-step backpropagation of errors and the usage of temporal memory in the network dynamics by means of decorrelation of activations. The BPDC rule is derived and theoretically justified from regarding learning as a constraint optimization problem and applies uniformly in discrete and continuous time. It is very easy to implement, and has a minimal complexity of 2N multiplications per time-step in the single output case. Nevertheless we obtain fast tracking and excellent performance in some benchmark problems including the Mackey-Glass time-series.},
	booktitle = {2004 {IEEE} {International} {Joint} {Conference} on {Neural} {Networks} ({IEEE} {Cat}. {No}.{04CH37541})},
	author = {Steil, J.J.},
	month = jul,
	year = {2004},
	note = {ISSN: 1098-7576},
	keywords = {Adaptive control, Backpropagation algorithms, Biological system modeling, Constraint optimization, Decorrelation, Information processing, Neurons, Recurrent neural networks, Reservoirs, Speech recognition},
	pages = {843--848 vol.2},
	file = {IEEE Xplore Abstract Record:/Users/tsmith/Zotero/storage/J7N6BS4T/1380039.html:text/html;Steil_2004_Backpropagation-decorrelation.pdf:/Users/tsmith/Drive/zotero/Steil_2004_Backpropagation-decorrelation.pdf:application/pdf},
}


@misc{chen_next_2022,
	title = {`{Next} {Generation}' {Reservoir} {Computing}: an {Empirical} {Data}-{Driven} {Expression} of {Dynamical} {Equations} in {Time}-{Stepping} {Form}},
	copyright = {All rights reserved},
	shorttitle = {`{Next} {Generation}' {Reservoir} {Computing}},
	url = {http://arxiv.org/abs/2201.05193},
	doi = {10.48550/arXiv.2201.05193},
	abstract = {Next generation reservoir computing based on nonlinear vector autoregression (NVAR) is applied to emulate simple dynamical system models and compared to numerical integration schemes such as Euler and the \$2{\textasciicircum}{\textbackslash}text\{nd\}\$ order Runge-Kutta. It is shown that the NVAR emulator can be interpreted as a data-driven method used to recover the numerical integration scheme that produced the data. It is also shown that the approach can be extended to produce high-order numerical schemes directly from data. The impacts of the presence of noise and temporal sparsity in the training set is further examined to gauge the potential use of this method for more realistic applications.},
	publisher = {arXiv},
	author = {Chen, Tse-Chun and Penny, Stephen G. and Smith, Timothy A. and Platt, Jason A.},
	month = jan,
	year = {2022},
	note = {arXiv:2201.05193 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Dynamical Systems, Mathematics - Numerical Analysis},
	annote = {Comment: 12 pages, 6 figures},
	file = {arXiv.org Snapshot:/Users/tsmith/Zotero/storage/IWYMQZRF/2201.html:text/html;Chen_et_al_2022_`Next_Generation'_Reservoir_Computing.pdf:/Users/tsmith/Drive/zotero/Chen_et_al_2022_`Next_Generation'_Reservoir_Computing.pdf:application/pdf},
}


@misc{zhang_catch-22_2022,
	title = {A {Catch}-22 of {Reservoir} {Computing}},
	url = {http://arxiv.org/abs/2210.10211},
	abstract = {Reservoir Computing (RC) is a simple and efficient model-free framework for data-driven predictions of nonlinear dynamical systems. Recently, Next Generation Reservoir Computing (NGRC) has emerged as an especially attractive variant of RC. By shifting the nonlinearity from the reservoir to the readout layer, NGRC requires less data and has fewer hyperparameters to optimize, making it suitable for challenging tasks such as predicting basins of attraction. Here, using paradigmatic multistable systems including magnetic pendulums and coupled Kuramoto oscillators, we show that the performance of NGRC models can be extremely sensitive to the choice of readout nonlinearity. In particular, by incorporating the exact nonlinearity from the original equations, NGRC trained on a single trajectory can predict pseudo-fractal basins with almost perfect accuracy. However, even a small uncertainty on the exact nonlinearity can completely break NGRC, rendering the prediction accuracy no better than chance. This creates a catch-22 for NGRC since it may not be able to make useful predictions unless a key part of the system being predicted (i.e., its nonlinearity) is already known. Our results highlight the challenges faced by data-driven methods in learning complex dynamical systems.},
	publisher = {arXiv},
	author = {Zhang, Yuanzhao and Cornelius, Sean P.},
	month = oct,
	year = {2022},
	note = {arXiv:2210.10211 [nlin]},
	keywords = {Computer Science - Machine Learning, Nonlinear Sciences - Chaotic Dynamics, Computer Science - Neural and Evolutionary Computing, Nonlinear Sciences - Adaptation and Self-Organizing Systems},
	annote = {Comment: Comments welcome. Our code can be found at https://github.com/spcornelius/NextGenRCBasins},
	file = {arXiv.org Snapshot:/Users/tsmith/Zotero/storage/ZXYGI25E/2210.html:text/html;Zhang_Cornelius_2022_A_Catch-22_of_Reservoir_Computing.pdf:/Users/tsmith/Drive/zotero/Zhang_Cornelius_2022_A_Catch-22_of_Reservoir_Computing.pdf:application/pdf},
}


@article{tulloch_note_2009,
	title = {A {Note} on the {Numerical} {Representation} of {Surface} {Dynamics} in {Quasigeostrophic} {Turbulence}: {Application} to the {Nonlinear} {Eady} {Model}},
	volume = {66},
	issn = {0022-4928, 1520-0469},
	shorttitle = {A {Note} on the {Numerical} {Representation} of {Surface} {Dynamics} in {Quasigeostrophic} {Turbulence}},
	url = {https://journals.ametsoc.org/view/journals/atsc/66/4/2008jas2921.1.xml},
	doi = {10.1175/2008JAS2921.1},
	abstract = {Abstract The quasigeostrophic equations consist of the advection of linearized potential vorticity coupled with advection of temperature at the bounding upper and lower surfaces. Numerical models of quasigeostrophic flow often employ greater (scaled) resolution in the horizontal than in the vertical (the two-layer model is an extreme example). In the interior, this has the effect of suppressing interactions between layers at horizontal scales that are small compared to Nδz/f (where δz is the vertical resolution, N the buoyancy frequency, and f the Coriolis parameter). The nature of the turbulent cascade in the interior is, however, not fundamentally altered because the downscale cascade of potential enstrophy in quasigeostrophic turbulence and the downscale cascade of enstrophy in two-dimensional turbulence (occurring layerwise) both yield energy spectra with slopes of −3. It is shown here that a similar restriction on the vertical resolution applies to the representation of horizontal motions at the surfaces, but the penalty for underresolving in the vertical is complete suppression of the surface temperature cascade at small scales and a corresponding artificial steepening of the surface energy spectrum. This effect is demonstrated in the nonlinear Eady model, using a finite-difference representation in comparison with a model that explicitly advects temperature at the upper and lower surfaces. Theoretical predictions for the spectrum of turbulence in the nonlinear Eady model are reviewed and compared to the simulated flows, showing that the latter model yields an accurate representation of the cascade dynamics. To accurately represent dynamics at horizontal wavenumber K in the vertically finite-differenced model, it is found that the vertical grid spacing must satisfy δz ≲ 0.3f/(NK); at wavenumbers K {\textgreater} 0.3f/(Nδz), the spectrum of temperature variance rolls off rapidly.},
	language = {EN},
	number = {4},
	journal = {Journal of the Atmospheric Sciences},
	author = {Tulloch, Ross and Smith, K. Shafer},
	month = apr,
	year = {2009},
	note = {Publisher: American Meteorological Society
Section: Journal of the Atmospheric Sciences},
	pages = {1063--1068},
	file = {Snapshot:/Users/tsmith/Zotero/storage/SXQCCMGM/2008jas2921.1.html:text/html;Tulloch_Smith_2009_A_Note_on_the_Numerical_Representation_of_Surface_Dynamics_in_Quasigeostrophic.pdf:/Users/tsmith/Drive/zotero/Tulloch_Smith_2009_A_Note_on_the_Numerical_Representation_of_Surface_Dynamics_in_Quasigeostrophic.pdf:application/pdf},
}

@article{blumen_uniform_1978,
	title = {{Uniform} {Potential} {Vorticity} {Flow}: {Part} {I}. {Theory} of {Wave} {Interactions} and {Two}-{Dimensional} {Turbulence}},
	volume = {35},
	issn = {0022-4928, 1520-0469},
	shorttitle = {Uniform {Potential} {Vorticity} {Flow}},
	url = {https://journals.ametsoc.org/view/journals/atsc/35/5/1520-0469_1978_035_0774_upvfpi_2_0_co_2.xml},
	doi = {10.1175/1520-0469(1978)035<0774:UPVFPI>2.0.CO;2},
	abstract = {Abstract Uniform potential vorticity flows are examined. In the quasi-geostrophic system, conservation of total energy and conservation of available potential energy on plane rigid horizontal boundaries imply a restriction on energy exchanges as a result of scale interactions. It is shown that for the Eady problem instability is always associated with energy transfer both up and down the vertical wavenumber spectrum although energy transfer from small to large three-dimensional wavenumbers may occur over a finite range of the spectrum. An inertial theory of two-dimensional turbulence is also presented. The formal analysis, based on Leith's diffusion approximation, predicts two inertial subranges: −5/3 and −1 power dependences on the horizontal wavenumber for available potential energy on horizontal boundaries. In the former range, available potential energy on horizontal boundaries cascades at a constant rate toward higher wavenumbers; in the latter range, the depth-integrated total energy cascades at a constant rate toward lower wavenumbers. Analysis of the semi-geostrophic equations, in the form presented by Hoskins, shows that a formal analogy exists between energy exchanges in this system and energy exchanges in the quasi-geostrophic system. The transformation back to physical space reveals that the mean strain rate, due to vertical wind shear, affects the complete spectrum of interacting waves. This latter result brings the concept of a local inertial energy transfer theory of turbulence for synoptic- and subsynoptic-scale motions into question, although it is concluded that further analysis and observational evidence would be required to resolve the problem.},
	language = {EN},
	number = {5},
	journal = {Journal of the Atmospheric Sciences},
	author = {Blumen, William},
	month = may,
	year = {1978},
	note = {Publisher: American Meteorological Society
Section: Journal of the Atmospheric Sciences},
	pages = {774--783},
	file = {Blumen_1978_Uniform_Potential_Vorticity_Flow.pdf:/Users/tsmith/Drive/zotero/Blumen_1978_Uniform_Potential_Vorticity_Flow.pdf:application/pdf;Snapshot:/Users/tsmith/Zotero/storage/3U9NJW34/1520-0469_1978_035_0774_upvfpi_2_0_co_2.html:text/html},
}

@article{blumen_uniform_1978-1,
	title = {{\noopsort{B}{Uniform}} {Potential} {Vorticity} {Flow}: {Part} {II}. {A} {Model} of {Wave} {Interacions}},
	volume = {35},
	issn = {0022-4928, 1520-0469},
	shorttitle = {Uniform {Potential} {Vorticity} {Flow}},
	url = {https://journals.ametsoc.org/view/journals/atsc/35/5/1520-0469_1978_035_0784_upvfpi_2_0_co_2.xml},
	doi = {10.1175/1520-0469(1978)035<0784:UPVFPI>2.0.CO;2},
	abstract = {Abstract Interactions between waves that satisfy uniform potential vorticity are considered. The formal analysis is restricted to a triad of eigenfunctions and the reduced system is constrained to satisfy conservation of total energy and conservation of available potential energy on plane rigid horizontal boundaries. A linear stability analysis is used to establish the properties of unstable waves in two cases: basic flows with anti-symmetry, and basic flows with symmetry in the vertical direction. A necessary condition for instability is that the vertical wavenumber of the basic flow must fall between the vertical wavenumbers associated with the perturbation waves. The properties of unstable waves in both cases are compared and analogies with the stability properties of the two-layer model are pointed out.},
	language = {EN},
	number = {5},
	journal = {Journal of the Atmospheric Sciences},
	author = {Blumen, William},
	month = may,
	year = {1978},
	note = {Publisher: American Meteorological Society
Section: Journal of the Atmospheric Sciences},
	pages = {784--789},
	file = {Blumen_1978_Uniform_Potential_Vorticity_Flow.pdf:/Users/tsmith/Drive/zotero/Blumen_1978_Uniform_Potential_Vorticity_Flow3.pdf:application/pdf},
}


@article{eady_long_1949,
	title = {Long {Waves} and {Cyclone} {Waves}},
	volume = {1},
	issn = {2153-3490},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.2153-3490.1949.tb01265.x},
	doi = {10.1111/j.2153-3490.1949.tb01265.x},
	abstract = {By obtaining complete solutions, satisfying all the relevant simultaneous differential equations and boundary conditions, representing small disturbances of simple states of steady baroclinic large-scale atmospheric motion it is shown that these simple states of motion are almost invariably unstable. An arbitrary disturbance (corresponding to some inhomogeneity of an actual system) may be regarded as analysed into “components” of a certain simple type, some of which grow exponentially with time. In all the cases examined there exists one particular component which grows faster than any other. It is shown how, by a process analogous to “natural selection”, this component becomes dominant in that almost any disturbance tends eventually to a definite size, structure and growth-rate (and to a characteristic life-history after the disturbance has ceased to be “small”), which depends only on the broad characteristics of the initial (unperturbed) system. The characteristic disturbances (forms of breakdown) of certain types of initial system (approximating to those observed in practice) are identified as the ideal forms of the observed cyclonc waves and long waves of middle and high latitudes. The implications regarding the ultimate limitations of weather forecasting are discussed.},
	language = {en},
	number = {3},
	journal = {Tellus},
	author = {Eady, E. T.},
	year = {1949},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.2153-3490.1949.tb01265.x},
	pages = {33--52},
	file = {Eady_1949_Long_Waves_and_Cyclone_Waves.pdf:/Users/tsmith/Drive/zotero/Eady_1949_Long_Waves_and_Cyclone_Waves.pdf:application/pdf;Snapshot:/Users/tsmith/Zotero/storage/R9XVLEAR/j.2153-3490.1949.tb01265.html:text/html},
}


@inproceedings{lorenz_predictability_1996,
	title = {Predictability - a problem partly solved},
	booktitle = {Proceedings of a {Seminar} {Held} at {ECMWF} on {Predictability}},
	author = {Lorenz, Edward},
	year = {1996},
	file = {Lorenz_1996_Predictability_-_a_problem_partly_solved.pdf:/Users/tsmith/Drive/zotero/Lorenz_1996_Predictability_-_a_problem_partly_solved.pdf:application/pdf},
}


@article{jones_efficient_1998,
	title = {Efficient {Global} {Optimization} of {Expensive} {Black}-{Box} {Functions}},
	volume = {13},
	issn = {1573-2916},
	url = {https://doi.org/10.1023/A:1008306431147},
	doi = {10.1023/A:1008306431147},
	abstract = {In many engineering optimization problems, the number of function evaluations is severely limited by time or cost. These problems pose a special challenge to the field of global optimization, since existing methods often require more function evaluations than can be comfortably afforded. One way to address this challenge is to fit response surfaces to data collected by evaluating the objective and constraint functions at a few points. These surfaces can then be used for visualization, tradeoff analysis, and optimization. In this paper, we introduce the reader to a response surface methodology that is especially good at modeling the nonlinear, multimodal functions that often occur in engineering. We then show how these approximating functions can be used to construct an efficient global optimization algorithm with a credible stopping rule. The key to using response surfaces for global optimization lies in balancing the need to exploit the approximating surface (by sampling where it is minimized) with the need to improve the approximation (by sampling where prediction error may be high). Striking this balance requires solving certain auxiliary problems which have previously been considered intractable, but we show how these computational obstacles can be overcome.},
	language = {en},
	number = {4},
	journal = {Journal of Global Optimization},
	author = {Jones, Donald R. and Schonlau, Matthias and Welch, William J.},
	month = dec,
	year = {1998},
	pages = {455--492},
	file = {Jones_et_al_1998_Efficient_Global_Optimization_of_Expensive_Black-Box_Functions.pdf:/Users/tsmith/Drive/zotero/Jones_et_al_1998_Efficient_Global_Optimization_of_Expensive_Black-Box_Functions.pdf:application/pdf},
}


@article{bouhlel_python_2019,
	title = {A {Python} surrogate modeling framework with derivatives},
	volume = {135},
	issn = {0965-9978},
	url = {https://www.sciencedirect.com/science/article/pii/S0965997818309360},
	doi = {10.1016/j.advengsoft.2019.03.005},
	abstract = {The surrogate modeling toolbox (SMT) is an open-source Python package that contains a collection of surrogate modeling methods, sampling techniques, and benchmarking functions. This package provides a library of surrogate models that is simple to use and facilitates the implementation of additional methods. SMT is different from existing surrogate modeling libraries because of its emphasis on derivatives, including training derivatives used for gradient-enhanced modeling, prediction derivatives, and derivatives with respect to training data. It also includes unique surrogate models: kriging by partial least-squares reduction, which scales well with the number of inputs; and energy-minimizing spline interpolation, which scales well with the number of training points. The efficiency and effectiveness of SMT are demonstrated through a series of examples. SMT is documented using custom tools for embedding automatically tested code and dynamically generated plots to produce high-quality user guides with minimal effort from contributors. SMT is maintained in a public version control repository.11https://github.com/SMTorg/SMT.},
	language = {en},
	journal = {Advances in Engineering Software},
	author = {Bouhlel, Mohamed Amine and Hwang, John T. and Bartoli, Nathalie and Lafage, Rémi and Morlier, Joseph and Martins, Joaquim R. R. A.},
	month = sep,
	year = {2019},
	keywords = {Derivatives, Gradient-enhanced surrogate modeling, Surrogate modeling},
	pages = {102662},
	file = {Bouhlel_et_al_2019_A_Python_surrogate_modeling_framework_with_derivatives.pdf:/Users/tsmith/Drive/zotero/Bouhlel_et_al_2019_A_Python_surrogate_modeling_framework_with_derivatives.pdf:application/pdf;ScienceDirect Snapshot:/Users/tsmith/Zotero/storage/5VHBKPYG/S0965997818309360.html:text/html},
}


@inproceedings{mockus_bayesian_1975,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {On bayesian methods for seeking the extremum},
	isbn = {978-3-540-37497-8},
	doi = {10.1007/3-540-07165-2_55},
	language = {en},
	booktitle = {Optimization {Techniques} {IFIP} {Technical} {Conference} {Novosibirsk}, {July} 1–7, 1974},
	publisher = {Springer},
	author = {Močkus, J.},
	editor = {Marchuk, G. I.},
	year = {1975},
	pages = {400--404},
	file = {Močkus_1975_On_bayesian_methods_for_seeking_the_extremum.pdf:/Users/tsmith/Drive/zotero/Močkus_1975_On_bayesian_methods_for_seeking_the_extremum.pdf:application/pdf},
}


@incollection{lukosevicius_practical_2012,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {A {Practical} {Guide} to {Applying} {Echo} {State} {Networks}},
	isbn = {978-3-642-35289-8},
	url = {https://doi.org/10.1007/978-3-642-35289-8_36},
	abstract = {Reservoir computing has emerged in the last decade as an alternative to gradient descent methods for training recurrent neural networks. Echo State Network (ESN) is one of the key reservoir computing “flavors”. While being practical, conceptually simple, and easy to implement, ESNs require some experience and insight to achieve the hailed good performance in many tasks. Here we present practical techniques and recommendations for successfully applying ESNs, as well as some more advanced application-specific modifications.},
	language = {en},
	booktitle = {Neural {Networks}: {Tricks} of the {Trade}: {Second} {Edition}},
	publisher = {Springer},
	author = {Lukoševičius, Mantas},
	editor = {Montavon, Grégoire and Orr, Geneviève B. and Müller, Klaus-Robert},
	year = {2012},
	doi = {10.1007/978-3-642-35289-8_36},
	keywords = {Little Mean Square, Output Feedback, Recurrent Neural Network, Ridge Regression, Spectral Radius},
	pages = {659--686},
	file = {Lukoševičius_2012_A_Practical_Guide_to_Applying_Echo_State_Networks.pdf:/Users/tsmith/Drive/zotero/Lukoševičius_2012_A_Practical_Guide_to_Applying_Echo_State_Networks.pdf:application/pdf},
}


@inproceedings{hermans_memory_2010,
	address = {Barcelona, Spain},
	title = {Memory in reservoirs for high dimensional input},
	isbn = {978-1-4244-6916-1},
	url = {http://ieeexplore.ieee.org/document/5596884/},
	doi = {10.1109/IJCNN.2010.5596884},
	abstract = {Reservoir Computing (RC) is a recently introduced scheme to employ recurrent neural networks while circumventing the difﬁculties that typically appear when training the recurrent weights. The ‘reservoir’ is a ﬁxed randomly initiated recurrent network which receives input via a random mapping. Only an instantaneous linear mapping from the network to the output is trained which can be done with linear regression. In this paper we study dynamical properties of reservoirs receiving a high number of inputs. More speciﬁcally, we investigate how the internal state of the network retains fading memory of its input signal. Memory properties for random recurrent networks have been thoroughly examined in past research, but only for one-dimensional input. Here we take into account statistics which will typically occur in high dimensional signals. We ﬁnd useful empirical data which expresses how memory in recurrent networks is distributed over the individual principal components of the input.},
	language = {en},
	booktitle = {The 2010 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	publisher = {IEEE},
	author = {Hermans, Michiel and Schrauwen, Benjamin},
	month = jul,
	year = {2010},
	pages = {1--7},
	file = {Hermans_Schrauwen_2010_Memory_in_reservoirs_for_high_dimensional_input.pdf:/Users/tsmith/Drive/zotero/Hermans_Schrauwen_2010_Memory_in_reservoirs_for_high_dimensional_input2.pdf:application/pdf},
}

@Manual{dask_2016,
  title = {Dask: Library for dynamic task scheduling},
  author = {{Dask Development Team}},
  year = {2016},
  url = {https://dask.org},
}


@misc{xu_overview_2022,
	title = {Overview frequency principle/spectral bias in deep learning},
	url = {http://arxiv.org/abs/2201.07395},
	abstract = {Understanding deep learning is increasingly emergent as it penetrates more and more into industry and science. In recent years, a research line from Fourier analysis sheds lights into this magical "black box" by showing a Frequency Principle (F-Principle or spectral bias) of the training behavior of deep neural networks (DNNs) -- DNNs often fit functions from low to high frequency during the training. The F-Principle is first demonstrated by one-dimensional synthetic data followed by the verification in high-dimensional real datasets. A series of works subsequently enhance the validity of the F-Principle. This low-frequency implicit bias reveals the strength of neural network in learning low-frequency functions as well as its deficiency in learning high-frequency functions. Such understanding inspires the design of DNN-based algorithms in practical problems, explains experimental phenomena emerging in various scenarios, and further advances the study of deep learning from the frequency perspective. Although incomplete, we provide an overview of F-Principle and propose some open problems for future research.},
	publisher = {arXiv},
	author = {Xu, Zhi-Qin John and Zhang, Yaoyu and Luo, Tao},
	month = jan,
	year = {2022},
	note = {arXiv:2201.07395 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/tsmith/Zotero/storage/WNZRG9Z5/2201.html:text/html;Xu_et_al_2022_Overview_frequency_principle-spectral_bias_in_deep_learning.pdf:/Users/tsmith/Drive/zotero/Xu_et_al_2022_Overview_frequency_principle-spectral_bias_in_deep_learning.pdf:application/pdf},
}


@misc{lam_graphcast_2022,
	title = {{GraphCast}: {Learning} skillful medium-range global weather forecasting},
	shorttitle = {{GraphCast}},
	url = {http://arxiv.org/abs/2212.12794},
	doi = {10.48550/arXiv.2212.12794},
	abstract = {We introduce a machine-learning (ML)-based weather simulator--called "GraphCast"--which outperforms the most accurate deterministic operational medium-range weather forecasting system in the world, as well as all previous ML baselines. GraphCast is an autoregressive model, based on graph neural networks and a novel high-resolution multi-scale mesh representation, which we trained on historical weather data from the European Centre for Medium-Range Weather Forecasts (ECMWF)'s ERA5 reanalysis archive. It can make 10-day forecasts, at 6-hour time intervals, of five surface variables and six atmospheric variables, each at 37 vertical pressure levels, on a 0.25-degree latitude-longitude grid, which corresponds to roughly 25 x 25 kilometer resolution at the equator. Our results show GraphCast is more accurate than ECMWF's deterministic operational forecasting system, HRES, on 90.0\% of the 2760 variable and lead time combinations we evaluated. GraphCast also outperforms the most accurate previous ML-based weather forecasting model on 99.2\% of the 252 targets it reported. GraphCast can generate a 10-day forecast (35 gigabytes of data) in under 60 seconds on Cloud TPU v4 hardware. Unlike traditional forecasting methods, ML-based forecasting scales well with data: by training on bigger, higher quality, and more recent data, the skill of the forecasts can improve. Together these results represent a key step forward in complementing and improving weather modeling with ML, open new opportunities for fast, accurate forecasting, and help realize the promise of ML-based simulation in the physical sciences.},
	publisher = {arXiv},
	author = {Lam, Remi and Sanchez-Gonzalez, Alvaro and Willson, Matthew and Wirnsberger, Peter and Fortunato, Meire and Pritzel, Alexander and Ravuri, Suman and Ewalds, Timo and Alet, Ferran and Eaton-Rosen, Zach and Hu, Weihua and Merose, Alexander and Hoyer, Stephan and Holland, George and Stott, Jacklynn and Vinyals, Oriol and Mohamed, Shakir and Battaglia, Peter},
	month = dec,
	year = {2022},
	note = {arXiv:2212.12794 [physics]},
	keywords = {Physics - Atmospheric and Oceanic Physics, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/tsmith/Zotero/storage/YQ4PULDK/2212.html:text/html;Lam_et_al_2022_GraphCast.pdf:/Users/tsmith/Drive/zotero/Lam_et_al_2022_GraphCast.pdf:application/pdf},
}


@misc{bi_pangu-weather_2022,
	title = {Pangu-{Weather}: {A} {3D} {High}-{Resolution} {Model} for {Fast} and {Accurate} {Global} {Weather} {Forecast}},
	shorttitle = {Pangu-{Weather}},
	url = {http://arxiv.org/abs/2211.02556},
	abstract = {In this paper, we present Pangu-Weather, a deep learning based system for fast and accurate global weather forecast. For this purpose, we establish a data-driven environment by downloading \$43\$ years of hourly global weather data from the 5th generation of ECMWF reanalysis (ERA5) data and train a few deep neural networks with about \$256\$ million parameters in total. The spatial resolution of forecast is \$0.25{\textasciicircum}{\textbackslash}circ{\textbackslash}times0.25{\textasciicircum}{\textbackslash}circ\$, comparable to the ECMWF Integrated Forecast Systems (IFS). More importantly, for the first time, an AI-based method outperforms state-of-the-art numerical weather prediction (NWP) methods in terms of accuracy (latitude-weighted RMSE and ACC) of all factors (e.g., geopotential, specific humidity, wind speed, temperature, etc.) and in all time ranges (from one hour to one week). There are two key strategies to improve the prediction accuracy: (i) designing a 3D Earth Specific Transformer (3DEST) architecture that formulates the height (pressure level) information into cubic data, and (ii) applying a hierarchical temporal aggregation algorithm to alleviate cumulative forecast errors. In deterministic forecast, Pangu-Weather shows great advantages for short to medium-range forecast (i.e., forecast time ranges from one hour to one week). Pangu-Weather supports a wide range of downstream forecast scenarios, including extreme weather forecast (e.g., tropical cyclone tracking) and large-member ensemble forecast in real-time. Pangu-Weather not only ends the debate on whether AI-based methods can surpass conventional NWP methods, but also reveals novel directions for improving deep learning weather forecast systems.},
	publisher = {arXiv},
	author = {Bi, Kaifeng and Xie, Lingxi and Zhang, Hengheng and Chen, Xin and Gu, Xiaotao and Tian, Qi},
	month = nov,
	year = {2022},
	note = {arXiv:2211.02556 [physics]},
	keywords = {Physics - Atmospheric and Oceanic Physics, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/tsmith/Zotero/storage/RMRJC5YC/2211.html:text/html;Bi_et_al_2022_Pangu-Weather.pdf:/Users/tsmith/Drive/zotero/Bi_et_al_2022_Pangu-Weather.pdf:application/pdf},
}


@misc{dosovitskiy_image_2021,
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	shorttitle = {An {Image} is {Worth} 16x16 {Words}},
	url = {http://arxiv.org/abs/2010.11929},
	abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
	publisher = {arXiv},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	month = jun,
	year = {2021},
	note = {arXiv:2010.11929 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/tsmith/Zotero/storage/ZFYMNYLF/2010.html:text/html;Dosovitskiy_et_al_2021_An_Image_is_Worth_16x16_Words.pdf:/Users/tsmith/Drive/zotero/Dosovitskiy_et_al_2021_An_Image_is_Worth_16x16_Words.pdf:application/pdf},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	year = {2017},
	file = {Vaswani_et_al_2017_Attention_is_All_you_Need.pdf:/Users/tsmith/Drive/zotero/Vaswani_et_al_2017_Attention_is_All_you_Need.pdf:application/pdf},
}


@misc{duncan_generative_2022,
	title = {Generative {Modeling} of {High}-resolution {Global} {Precipitation} {Forecasts}},
	url = {http://arxiv.org/abs/2210.12504},
	doi = {10.48550/arXiv.2210.12504},
	abstract = {Forecasting global precipitation patterns and, in particular, extreme precipitation events is of critical importance to preparing for and adapting to climate change. Making accurate high-resolution precipitation forecasts using traditional physical models remains a major challenge in operational weather forecasting as they incur substantial computational costs and struggle to achieve sufficient forecast skill. Recently, deep-learning-based models have shown great promise in closing the gap with numerical weather prediction (NWP) models in terms of precipitation forecast skill, opening up exciting new avenues for precipitation modeling. However, it is challenging for these deep learning models to fully resolve the fine-scale structures of precipitation phenomena and adequately characterize the extremes of the long-tailed precipitation distribution. In this work, we present several improvements to the architecture and training process of a current state-of-the art deep learning precipitation model (FourCastNet) using a novel generative adversarial network (GAN) to better capture fine scales and extremes. Our improvements achieve superior performance in capturing the extreme percentiles of global precipitation, while comparable to state-of-the-art NWP models in terms of forecast skill at 1--2 day lead times. Together, these improvements set a new state-of-the-art in global precipitation forecasting.},
	publisher = {arXiv},
	author = {Duncan, James and Subramanian, Shashank and Harrington, Peter},
	month = oct,
	year = {2022},
	note = {arXiv:2210.12504 [physics]},
	keywords = {Physics - Atmospheric and Oceanic Physics, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/tsmith/Zotero/storage/X5ISCBNJ/2210.html:text/html;Duncan_et_al_2022_Generative_Modeling_of_High-resolution_Global_Precipitation_Forecasts.pdf:/Users/tsmith/Drive/zotero/Duncan_et_al_2022_Generative_Modeling_of_High-resolution_Global_Precipitation_Forecasts.pdf:application/pdf},
}


@inproceedings{basterrech_self-organizing_2011,
	title = {Self-{Organizing} {Maps} and {Scale}-{Invariant} {Maps} in {Echo} {State} {Networks}},
	doi = {10.1109/ISDA.2011.6121637},
	abstract = {In the last years a new approach for designing and training artificial Recurrent Neural Network (RNN) have been investigated under the name of Reservoir Computing (RC). One important model in the field of RC has been developed under the name of Echo State Networks (ESNs). Traditionally, an ESN uses a RNN with random untrained parameters called the reservoir. The Self-Organizing Map (SOM) and the Scale Invariant Map (SIM) are two methods of topographic maps which have been used in different tasks of unsupervised learning. Recently, new works show that is effective using the SOM to set values of the reservoir parameters. The primary goal of this work is to improve the performance of ESN using the another method SIM. Here, we present the description of these two topographic map methods and the way to apply its on the ESN initialization. We specify an original algorithm to set the reservoir weights using the SOM and SIM. Furthermore, we use artificial data set to compare the use of topographic maps to initialize the ESN with random initialization. Overall, our results show the aptitude of SIM and SOM to set the reservoir parameters.},
	booktitle = {2011 11th {International} {Conference} on {Intelligent} {Systems} {Design} and {Applications}},
	author = {Basterrech, Sebastián and Fyfe, Colin and Rubino, Gerardo},
	month = nov,
	year = {2011},
	note = {ISSN: 2164-7151},
	keywords = {Recurrent neural networks, Reservoir Computing, Self-Organizing Maps, Neurons, Reservoirs, Training, Training data, Echo State Networks, Scale Invariant Maps, Self organizing feature maps, Time series analysis, Times Series Prediction, Topographic Maps},
	pages = {94--99},
	file = {Basterrech_et_al_2011_Self-Organizing_Maps_and_Scale-Invariant_Maps_in_Echo_State_Networks.pdf:/Users/tsmith/Drive/zotero/Basterrech_et_al_2011_Self-Organizing_Maps_and_Scale-Invariant_Maps_in_Echo_State_Networks.pdf:application/pdf},
}


@article{heyder_generalizability_2022,
	title = {Generalizability of reservoir computing for flux-driven two-dimensional convection},
	volume = {106},
	url = {https://link.aps.org/doi/10.1103/PhysRevE.106.055303},
	doi = {10.1103/PhysRevE.106.055303},
	abstract = {We explore the generalization properties of an echo state network applied as a reduced-order model to predict flux-driven two-dimensional turbulent convection. To this end, we consider a convection domain with constant height with a variable ratio of buoyancy fluxes at the top and bottom boundaries, which break the top-down symmetry in comparison to the standard Rayleigh-Bénard case, thus leading to highly asymmetric mean and fluctuation profiles across the layer. Our direct numerical simulation model describes a convective boundary layer in a simple way. The data are used to train and test a recurrent neural network in the form of an echo state network. The input of the echo state network is obtained in two different ways, either by a proper orthogonal decomposition or by a convolutional autoencoder. In both cases, the echo state network reproduces the turbulence dynamics and the statistical properties of the buoyancy flux, and is able to model unseen data records with different flux ratios.},
	number = {5},
	journal = {Physical Review E},
	author = {Heyder, Florian and Mellado, Juan Pedro and Schumacher, Jörg},
	month = nov,
	year = {2022},
	note = {Publisher: American Physical Society},
	pages = {055303},
	file = {APS Snapshot:/Users/tsmith/Zotero/storage/9V3BD48N/PhysRevE.106.html:text/html;Heyder_et_al_2022_Generalizability_of_reservoir_computing_for_flux-driven_two-dimensional.pdf:/Users/tsmith/Drive/zotero/Heyder_et_al_2022_Generalizability_of_reservoir_computing_for_flux-driven_two-dimensional.pdf:application/pdf},
}


@misc{jordanou_investigation_2022,
	title = {Investigation of {Proper} {Orthogonal} {Decomposition} for {Echo} {State} {Networks}},
	url = {http://arxiv.org/abs/2211.17179},
	abstract = {Echo State Networks (ESN) are a type of Recurrent Neural Networks that yields promising results in representing time series and nonlinear dynamic systems. Although they are equipped with a very efficient training procedure, Reservoir Computing strategies, such as the ESN, require the use of high order networks, i.e. large number of layers, resulting in number of states that is magnitudes higher than the number of model inputs and outputs. This not only makes the computation of a time step more costly, but also may pose robustness issues when applying ESNs to problems such as Model Predictive Control (MPC) and other optimal control problems. One such way to circumvent this is through Model Order Reduction strategies such as the Proper Orthogonal Decomposition (POD) and its variants (POD-DEIM), whereby we find an equivalent lower order representation to an already trained high dimension ESN. The objective of this work is to investigate and analyze the performance of POD methods in Echo State Networks, evaluating their effectiveness. To this end, we evaluate the Memory Capacity (MC) of the POD-reduced network in comparison to the original (full order) ENS. We also perform experiments on two different numerical case studies: a NARMA10 difference equation and an oil platform containing two wells and one riser. The results show that there is little loss of performance comparing the original ESN to a POD-reduced counterpart, and also that the performance of a POD-reduced ESN tend to be superior to a normal ESN of the same size. Also we attain speedups of around \$80{\textbackslash}\%\$ in comparison to the original ESN.},
	publisher = {arXiv},
	author = {Jordanou, Jean Panaioti and Antonelo, Eric Aislan and Camponogara, Eduardo and Gildin, Eduardo},
	month = dec,
	year = {2022},
	note = {arXiv:2211.17179 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Electrical Engineering and Systems Science - Systems and Control},
	file = {arXiv.org Snapshot:/Users/tsmith/Zotero/storage/VQ2KX46Q/2211.html:text/html;Jordanou_et_al_2022_Investigation_of_Proper_Orthogonal_Decomposition_for_Echo_State_Networks.pdf:/Users/tsmith/Drive/zotero/Jordanou_et_al_2022_Investigation_of_Proper_Orthogonal_Decomposition_for_Echo_State_Networks.pdf:application/pdf},
}


@article{whiteaker_reducing_2022,
	title = {Reducing echo state network size with controllability matrices},
	volume = {32},
	issn = {1054-1500},
	url = {https://aip.scitation.org/doi/full/10.1063/5.0071926},
	doi = {10.1063/5.0071926},
	abstract = {Echo state networks are a fast training variant of recurrent neural networks excelling at approximating nonlinear dynamical systems and time series prediction. These machine learning models act as nonlinear fading memory filters. While these models benefit from quick training and low complexity, computation demands from a large reservoir matrix are a bottleneck. Using control theory, a reduced size replacement reservoir matrix is found. Starting from a large, task-effective reservoir matrix, we form a controllability matrix whose rank indicates the active sub-manifold and candidate replacement reservoir size. Resulting time speed-ups and reduced memory usage come with minimal error increase to chaotic climate reconstruction or short term prediction. Experiments are performed on simple time series signals and the Lorenz-1963 and Mackey–Glass complex chaotic signals. Observing low error models shows variation of active rank and memory along a sequence of predictions.},
	number = {7},
	journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
	author = {Whiteaker, Brian and Gerstoft, Peter},
	month = jul,
	year = {2022},
	note = {Publisher: American Institute of Physics},
	pages = {073116},
	file = {Whiteaker_Gerstoft_2022_Reducing_echo_state_network_size_with_controllability_matrices.pdf:/Users/tsmith/Drive/zotero/Whiteaker_Gerstoft_2022_Reducing_echo_state_network_size_with_controllability_matrices.pdf:application/pdf},
}


@article{ma_deepr-esn_2020,
	title = {{DeePr}-{ESN}: {A} deep projection-encoding echo-state network},
	volume = {511},
	issn = {0020-0255},
	shorttitle = {{DeePr}-{ESN}},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025519309053},
	doi = {10.1016/j.ins.2019.09.049},
	abstract = {As a recurrent neural network that requires no training, the reservoir computing (RC) model has attracted widespread attention in the last decade, especially in the context of time series prediction. However, most time series have a multiscale structure, which a single-hidden-layer RC model may have difficulty capturing. In this paper, we propose a novel multiple projection-encoding hierarchical reservoir computing framework called Deep Projection-encoding Echo State Network (DeePr-ESN). The most distinctive feature of our model is its ability to learn multiscale dynamics through stacked ESNs, connected via subspace projections. Specifically, when an input time series is projected into the high-dimensional echo-state space of a reservoir, a subsequent encoding layer (e.g., an autoencoder or PCA) projects the echo-state representations into a lower-dimensional feature space. These representations are the principal components of the echo-state representations, which removes the high frequency components of the representations. These can then be processed by another ESN through random connections. By using projection layers and encoding layers alternately, our DeePr-ESN can provide much more robust generalization performance than previous methods, and also fully takes advantage of the temporal kernel property of ESNs to encode the multiscale dynamics of time series. In our experiments, the DeePr-ESNs outperform both standard ESNs and existing hierarchical reservoir computing models on some artificial and real-world time series prediction tasks.},
	language = {en},
	journal = {Information Sciences},
	author = {Ma, Qianli and Shen, Lifeng and Cottrell, Garrison W.},
	month = feb,
	year = {2020},
	keywords = {Echo state network, Hierarchical reservoir computing, Multiscale dynamics, Time series prediction},
	pages = {152--171},
	file = {Ma_et_al_2020_DeePr-ESN.pdf:/Users/tsmith/Drive/zotero/Ma_et_al_2020_DeePr-ESN.pdf:application/pdf;ScienceDirect Snapshot:/Users/tsmith/Zotero/storage/Z3EZH8JD/S0020025519309053.html:text/html},
}


@article{moon_hierarchical_2021,
	title = {Hierarchical architectures in reservoir computing systems},
	volume = {1},
	issn = {2634-4386},
	url = {https://doi.org/10.1088/2634-4386/ac1b75},
	doi = {10.1088/2634-4386/ac1b75},
	abstract = {Reservoir computing (RC) offers efficient temporal data processing with a low training cost by separating recurrent neural networks into a fixed network with recurrent connections and a trainable linear network. The quality of the fixed network, called reservoir, is the most important factor that determines the performance of the RC system. In this paper, we investigate the influence of the hierarchical reservoir structure on the properties of the reservoir and the performance of the RC system. Analogous to deep neural networks, stacking sub-reservoirs in series is an efficient way to enhance the nonlinearity of data transformation to high-dimensional space and expand the diversity of temporal information captured by the reservoir. These deep reservoir systems offer better performance when compared to simply increasing the size of the reservoir or the number of sub-reservoirs. Low frequency components are mainly captured by the sub-reservoirs in later stage of the deep reservoir structure, similar to observations that more abstract information can be extracted by layers in the late stage of deep neural networks. When the total size of the reservoir is fixed, tradeoff between the number of sub-reservoirs and the size of each sub-reservoir needs to be carefully considered, due to the degraded ability of individual sub-reservoirs at small sizes. Improved performance of the deep reservoir structure alleviates the difficulty of implementing the RC system on hardware systems.},
	language = {en},
	number = {1},
	journal = {Neuromorphic Computing and Engineering},
	author = {Moon, John and Wu, Yuting and Lu, Wei D.},
	month = aug,
	year = {2021},
	note = {Publisher: IOP Publishing},
	pages = {014006},
	file = {Moon_et_al_2021_Hierarchical_architectures_in_reservoir_computing_systems.pdf:/Users/tsmith/Drive/zotero/Moon_et_al_2021_Hierarchical_architectures_in_reservoir_computing_systems.pdf:application/pdf},
}

@article{gallicchio_design_2018,
	title = {Design of deep echo state networks},
	volume = {108},
	issn = {0893-6080},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608018302223},
	doi = {10.1016/j.neunet.2018.08.002},
	abstract = {In this paper, we provide a novel approach to the architectural design of deep Recurrent Neural Networks using signal frequency analysis. In particular, focusing on the Reservoir Computing framework and inspired by the principles related to the inherent effect of layering, we address a fundamental open issue in deep learning, namely the question of how to establish the number of layers in recurrent architectures in the form of deep echo state networks (DeepESNs). The proposed method is first analyzed and refined on a controlled scenario and then it is experimentally assessed on challenging real-world tasks. The achieved results also show the ability of properly designed DeepESNs to outperform RC approaches on a speech recognition task, and to compete with the state-of-the-art in time-series prediction on polyphonic music tasks.},
	language = {en},
	journal = {Neural Networks},
	author = {Gallicchio, Claudio and Micheli, Alessio and Pedrelli, Luca},
	month = dec,
	year = {2018},
	keywords = {Architectural design of recurrent neural networks, Deep echo state networks, Deep recurrent neural networks, Echo state networks, Reservoir computing},
	pages = {33--47},
	file = {Gallicchio_et_al_2018_Design_of_deep_echo_state_networks.pdf:/Users/tsmith/Drive/zotero/Gallicchio_et_al_2018_Design_of_deep_echo_state_networks.pdf:application/pdf;ScienceDirect Snapshot:/Users/tsmith/Zotero/storage/MWDJ8BU8/S0893608018302223.html:text/html},
}

@article{gallicchio_deep_2017,
	series = {Advances in artificial neural networks, machine learning and computational intelligence},
	title = {Deep reservoir computing: {A} critical experimental analysis},
	volume = {268},
	issn = {0925-2312},
	shorttitle = {Deep reservoir computing},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231217307567},
	doi = {10.1016/j.neucom.2016.12.089},
	abstract = {In this paper, we propose an empirical analysis of deep recurrent neural network (RNN) architectures with stacked layers. The main aim is to address some fundamental open research issues on the significance of creating deep layered architectures in RNN and to characterize the inherent hierarchical representation of time in such models, especially for efficient implementations. In particular, the analysis aims at the study and proposal of approaches to develop and enhance hierarchical dynamics in deep architectures within the efficient Reservoir Computing (RC) framework for RNN modeling. The effect of a deep layered organization of RC models is investigated in terms of both occurrence of multiple time-scale and increasing of richness of the dynamics. It turns out that a deep layering of recurrent models allows an effective diversification of temporal representations in the layers of the hierarchy, by amplifying the effects of the factors influencing the time-scales and the richness of the dynamics, measured as the entropy of recurrent units activations. The advantages of the proposed approach are also highlighted by measuring the increment of the short-term memory capacity of the RC models.},
	language = {en},
	journal = {Neurocomputing},
	author = {Gallicchio, Claudio and Micheli, Alessio and Pedrelli, Luca},
	month = dec,
	year = {2017},
	keywords = {Deep Learning, Deep neural networks, Echo State Networks, Multiple time-scale dynamics, Recurrent neural networks, Reservoir computing},
	pages = {87--99},
	file = {Gallicchio_et_al_2017_Deep_reservoir_computing.pdf:/Users/tsmith/Drive/zotero/Gallicchio_et_al_2017_Deep_reservoir_computing.pdf:application/pdf;ScienceDirect Snapshot:/Users/tsmith/Zotero/storage/NF22976E/S0925231217307567.html:text/html},
}

@article{malik_multilayered_2017,
	title = {Multilayered {Echo} {State} {Machine}: {A} {Novel} {Architecture} and {Algorithm}},
	volume = {47},
	issn = {2168-2267, 2168-2275},
	shorttitle = {Multilayered {Echo} {State} {Machine}},
	url = {https://ieeexplore.ieee.org/document/7494974/},
	doi = {10.1109/TCYB.2016.2533545},
	abstract = {In this paper, we present a novel architecture and learning algorithm for a multi-layered Echo State Machine (MLESM). Traditional Echo state networks (ESN) refer to a particular type of Reservoir Computing (RC) architecture. They constitute an effective approach to recurrent neural network (RNN) training, with the (RNN-based) reservoir generated randomly, and only the readout trained using a simple computationally efﬁcient algorithm. ESNs have greatly facilitated the real-time application of RNN, and have been shown to outperform classical approaches in a number of benchmark tasks. In this paper, we introduce novel criteria for integrating multiple layers of reservoirs within an echo state machine, with the resulting architecture termed the ML-ESM. The addition of multiple layers of reservoirs are shown to provide a more robust alternative to conventional reservoir computing networks. We demonstrate the comparative merits of this approach in a number of applications, considering both benchmark datasets and real world applications.},
	language = {en},
	number = {4},
	journal = {IEEE Transactions on Cybernetics},
	author = {Malik, Zeeshan Khawar and Hussain, Amir and Wu, Qingming Jonathan},
	month = apr,
	year = {2017},
	pages = {946--959},
	file = {Malik_et_al_2017_Multilayered_Echo_State_Machine.pdf:/Users/tsmith/Drive/zotero/Malik_et_al_2017_Multilayered_Echo_State_Machine.pdf:application/pdf},
}

@inproceedings{cupy_learningsys2017,
  author       = "Okuta, Ryosuke and Unno, Yuya and Nishino, Daisuke and Hido, Shohei and Loomis, Crissman",
  title        = "CuPy: A NumPy-Compatible Library for NVIDIA GPU Calculations",
  booktitle    = "Proceedings of Workshop on Machine Learning Systems (LearningSys) in The Thirty-first Annual Conference on Neural Information Processing Systems (NIPS)",
  year         = "2017",
  url          = "http://learningsys.org/nips17/assets/papers/paper_16.pdf"
}

@misc{gom_dataset,
    author  = "{HYCOM}",
    title   = "{HYCOM} + {NCODA} {Gulf} of {Mexico} {1/25°} {Reanalysis}, ({GOM}u0.04/expt{\_}50.1)",
    year    = "2016",
    note    = "Data retrieved from HYCOM, \url{https://www.hycom.org/data/gomu0pt04/expt-50pt1}",
}


@incollection{rossa_overview_2008,
	address = {Berlin, Heidelberg},
	title = {Overview of methods for the verification of quantitative precipitation forecasts},
	isbn = {978-3-540-77655-0},
	url = {https://doi.org/10.1007/978-3-540-77655-0_16},
	language = {en},
	booktitle = {Precipitation: {Advances} in {Measurement}, {Estimation} and {Prediction}},
	publisher = {Springer},
	author = {Rossa, Andrea and Nurmi, Pertti and Ebert, Elizabeth},
	editor = {Michaelides, Silas},
	year = {2008},
	doi = {10.1007/978-3-540-77655-0_16},
	keywords = {Equitable Threat Score, Numerical Weather Prediction Model, Precipitation Forecast, Relative Operating Characteristic, Skill Score},
	pages = {419--452},
	file = {Rossa_et_al_2008_Overview_of_methods_for_the_verification_of_quantitative_precipitation_forecasts.pdf:/Users/tsmith/Drive/zotero/Rossa_et_al_2008_Overview_of_methods_for_the_verification_of_quantitative_precipitation_forecasts.pdf:application/pdf},
}


@article{held_surface_1995,
	title = {Surface quasi-geostrophic dynamics},
	volume = {282},
	issn = {1469-7645, 0022-1120},
	url = {http://www.cambridge.org/core/journals/journal-of-fluid-mechanics/article/surface-quasigeostrophic-dynamics/81CC9FC82F189A1E59E7816F47D3260F},
	doi = {10.1017/S0022112095000012},
	abstract = {The dynamics of quasi-geostrophic flow with uniform potential vorticity reduces to the evolution of buoyancy, or potential temperature, on horizontal boundaries. There is a formal resemblance to two-dimensional flow, with surface temperature playing the role of vorticity, but a different relationship between the flow and the advected scalar creates several distinctive features. A series of examples are described which highlight some of these features: the evolution of an elliptical vortex; the start-up vortex shed by flow over a mountain; the instability of temperature filaments; the ‘edge wave’ critical layer; and mixing in an overturning edge wave. Characteristics of the direct cascade of the tracer variance to small scales in homogeneous turbulence, as well as the inverse energy cascade, are also described. In addition to its geophysical relevance, the ubiquitous generation of secondary instabilities and the possibility of finite-time collapse make this system a potentially important, numerically tractable, testbed for turbulence theories.},
	language = {en},
	journal = {Journal of Fluid Mechanics},
	author = {Held, Isaac M. and Pierrehumbert, Raymond T. and Garner, Stephen T. and Swanson, Kyle L.},
	month = jan,
	year = {1995},
	note = {Publisher: Cambridge University Press},
	pages = {1--20},
	file = {Held_et_al_1995_Surface_quasi-geostrophic_dynamics.pdf:/Users/tsmith/Drive/zotero/Held_et_al_1995_Surface_quasi-geostrophic_dynamics.pdf:application/pdf;Snapshot:/Users/tsmith/Zotero/storage/JCSLGIDA/81CC9FC82F189A1E59E7816F47D3260F.html:text/html},
}


@article{li_data-based_2019,
	title = {Data-{Based} {Approach} for {Fast} {Airfoil} {Analysis} and {Optimization}},
	volume = {57},
	issn = {0001-1452},
	url = {https://arc.aiaa.org/doi/10.2514/1.J057129},
	doi = {10.2514/1.J057129},
	abstract = {Airfoils are of great importance in aerodynamic design, and various tools have been developed to evaluate and optimize their performance. Existing tools are usually either accurate or efficient, but not both. This paper presents a tool that can analyze airfoils in both subsonic and transonic regimes in about one-hundredth of a second, and optimize airfoil shapes in a few seconds. Camber and thickness mode shapes derived from over 1000 existing airfoils are used to parameterize the airfoil shape, which reduces the number of design variables. More than 100,000 Reynolds-averaged Navier–Stokes (RANS) evaluations associated with different airfoils and flow conditions are used to train a surrogate model that combines gradient-enhanced kriging, partial least squares, and mixture of experts. These surrogate models provide fast aerodynamic analysis and gradient computation, which are coupled with a gradient-based optimizer to perform rapid airfoil shape design optimization. When comparing the surrogate-based optimization with optimization based on direct RANS evaluations, the largest differences in minimum 
𝐶
𝑑
𝐶
𝑑
 are 0.04 counts for subsonic cases and 2.5 counts for transonic cases. This approach opens the door for interactive airfoil analysis and design optimization using any modern computer or mobile device.},
	number = {2},
	journal = {AIAA Journal},
	author = {Li, Jichao and Bouhlel, Mohamed Amine and Martins, Joaquim R. R. A.},
	month = feb,
	year = {2019},
	note = {Publisher: American Institute of Aeronautics and Astronautics},
	keywords = {Aerodynamic Shape Optimization, Airfoil Geometry, CFD Simulation, Flow Conditions, Gradient Free Optimization, Lift Coefficient, Numerical Modeling, Reynolds Averaged Navier Stokes, Singular Value Decomposition, Supercritical Airfoils},
	pages = {581--596},
	file = {Li_et_al_2019_Data-Based_Approach_for_Fast_Airfoil_Analysis_and_Optimization.pdf:/Users/tsmith/Drive/zotero/Li_et_al_2019_Data-Based_Approach_for_Fast_Airfoil_Analysis_and_Optimization.pdf:application/pdf},
}

@article{bouhlel_scalable_2020,
	title = {Scalable gradient–enhanced artificial neural networks for airfoil shape design in the subsonic and transonic regimes},
	volume = {61},
	issn = {1615-1488},
	url = {https://doi.org/10.1007/s00158-020-02488-5},
	doi = {10.1007/s00158-020-02488-5},
	abstract = {Airfoil shape design is one of the most fundamental elements in aircraft design. Existing airfoil design tools require at least a few minutes to analyze a new shape and hours to perform shape optimization. To drastically reduce the computational time of both analysis and design optimization, we use machine learning to create a model of a wide range of possible airfoils at a range of flight conditions, making it possible to perform airfoil design optimization in a few seconds. The machine learning consists of gradient-enhanced artificial neural networks where the gradient information is phased in gradually. This new gradient-enhanced artificial neural network approach is trained to model the aerodynamic force coefficients of airfoils in both subsonic and transonic regimes. The aerodynamics is modeled with Reynolds-averaged Navier–Stokes (RANS)-based computational fluid dynamics (CFD). The proposed approach outperforms an existing airfoil model that uses a mixture of experts technique combined with a gradient-based kriging surrogate model. The approach yields to similar airfoil shape optimization solutions than high-fidelity CFD optimization solutions with a difference of 0.01 count and 0.12 count for Cd in subsonic and transonic regimes, respectively. Airfoil optimization problems are solved in a few seconds (instead of hours using CFD-based optimization), making the design process much more interactive, as demonstrated in the Webfoil airfoil design optimization tool.},
	language = {en},
	number = {4},
	journal = {Structural and Multidisciplinary Optimization},
	author = {Bouhlel, Mohamed Amine and He, Sicheng and Martins, Joaquim R. R. A.},
	month = apr,
	year = {2020},
	keywords = {Airfoils, Artificial neural networks, Gradient-enhanced modeling, Surrogate modeling},
	pages = {1363--1376},
	file = {Bouhlel_et_al_2020_Scalable_gradient–enhanced_artificial_neural_networks_for_airfoil_shape_design.pdf:/Users/tsmith/Drive/zotero/Bouhlel_et_al_2020_Scalable_gradient–enhanced_artificial_neural_networks_for_airfoil_shape_design.pdf:application/pdf},
}


@misc{perezhogin_generative_2023,
	title = {Generative data-driven approaches for stochastic subgrid parameterizations in an idealized ocean model},
	url = {http://arxiv.org/abs/2302.07984},
	abstract = {Subgrid parameterizations of mesoscale eddies continue to be in demand for climate simulations. These subgrid parameterizations can be powerfully designed using physics and/or data-driven methods, with uncertainty quantification. For example, Guillaumin and Zanna (2021) proposed a Machine Learning (ML) model that predicts subgrid forcing and its local uncertainty. The major assumption and potential drawback of this model is the statistical independence of stochastic residuals between grid points. Here, we aim to improve the simulation of stochastic forcing with generative models of ML, such as Generative adversarial network (GAN) and Variational autoencoder (VAE). Generative models learn the distribution of subgrid forcing conditioned on the resolved flow directly from data and they can produce new samples from this distribution. Generative models can potentially capture not only the spatial correlation but any statistically significant property of subgrid forcing. We test the proposed stochastic parameterizations offline and online in an idealized ocean model. We show that generative models are able to predict subgrid forcing and its uncertainty with spatially correlated stochastic forcing. Online simulations for a range of resolutions demonstrated that generative models are superior to the baseline ML model at the coarsest resolution.},
	publisher = {arXiv},
	author = {Perezhogin, Pavel and Zanna, Laure and Fernandez-Granda, Carlos},
	month = feb,
	year = {2023},
	note = {arXiv:2302.07984 [physics]},
	keywords = {Physics - Atmospheric and Oceanic Physics},
	file = {arXiv.org Snapshot:/Users/tsmith/Zotero/storage/JACT3NSC/2302.html:text/html;Perezhogin_et_al_2023_Generative_data-driven_approaches_for_stochastic_subgrid_parameterizations_in.pdf:/Users/tsmith/Drive/zotero/Perezhogin_et_al_2023_Generative_data-driven_approaches_for_stochastic_subgrid_parameterizations_in.pdf:application/pdf},
}

@article{kalnay_ensemble_2006,
  title={Ensemble forecasting and data assimilation: two problems with the same solution},
  author={Kalnay, Eugenia and Hunt, Brian and Ott, Edward and Szunyogh, Istvan},
  journal={Predictability of weather and climate},
  volume={157},
  pages={180},
  year={2006},
  publisher={Citeseer}
}


@article{orlanski_rational_1975,
	title = {A {Rational} {Subdivision} of {Scales} for {Atmospheric} {Processes}},
	volume = {56},
	issn = {0003-0007},
	url = {https://www.jstor.org/stable/26216020},
	abstract = {Some atmospheric scale definitions are reviewed and a proposed new subdivision of scales that covers the entire spectrum is described.},
	number = {5},
	journal = {Bulletin of the American Meteorological Society},
	author = {Orlanski, Isidoro},
	year = {1975},
	note = {Publisher: American Meteorological Society},
	pages = {527--530},
	file = {Orlanski_1975_A_Rational_Subdivision_of_Scales_for_Atmospheric_Processes.pdf:/Users/tsmith/Drive/zotero/Orlanski_1975_A_Rational_Subdivision_of_Scales_for_Atmospheric_Processes.pdf:application/pdf},
}


@misc{platt_constraining_2023,
	title = {Constraining {Chaos}: {Enforcing} dynamical invariants in the training of recurrent neural networks},
	copyright = {All rights reserved},
	shorttitle = {Constraining {Chaos}},
	url = {http://arxiv.org/abs/2304.12865},
	doi = {10.48550/arXiv.2304.12865},
	abstract = {Drawing on ergodic theory, we introduce a novel training method for machine learning based forecasting methods for chaotic dynamical systems. The training enforces dynamical invariants--such as the Lyapunov exponent spectrum and fractal dimension--in the systems of interest, enabling longer and more stable forecasts when operating with limited data. The technique is demonstrated in detail using the recurrent neural network architecture of reservoir computing. Results are given for the Lorenz 1996 chaotic dynamical system and a spectral quasi-geostrophic model, both typical test cases for numerical weather prediction.},
	publisher = {arXiv},
	author = {Platt, Jason A. and Penny, Stephen G. and Smith, Timothy A. and Chen, Tse-Chun and Abarbanel, Henry D. I.},
	month = apr,
	year = {2023},
	note = {arXiv:2304.12865 [physics]},
	keywords = {Computer Science - Machine Learning, Mathematics - Dynamical Systems, Physics - Geophysics},
	file = {arXiv.org Snapshot:/Users/tsmith/Zotero/storage/ZQW7KU3N/2304.html:text/html;Platt_et_al_2023_Constraining_Chaos.pdf:/Users/tsmith/Drive/zotero/Platt_et_al_2023_Constraining_Chaos.pdf:application/pdf},
}

@software{smith_rcgfd_2023,
  author       = {Timothy Smith},
  title        = {timothyas/rc-gfd: Revision 1},
  month        = sep,
  year         = 2023,
  publisher    = {Zenodo},
  version      = {v1},
  doi          = {10.5281/zenodo.8368225},
  url          = {https://doi.org/10.5281/zenodo.8368225}
}


@misc{chattopadhyay_long-term_2023,
	title = {Long-term instabilities of deep learning-based digital twins of the climate system: {The} cause and a solution},
	shorttitle = {Long-term instabilities of deep learning-based digital twins of the climate system},
	url = {http://arxiv.org/abs/2304.07029},
	publisher = {arXiv},
	author = {Chattopadhyay, Ashesh and Hassanzadeh, Pedram},
	month = apr,
	year = {2023},
	note = {arXiv:2304.07029 [physics]},
	keywords = {Physics - Atmospheric and Oceanic Physics, Mathematics - Numerical Analysis, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Physics - Fluid Dynamics},
	file = {arXiv.org Snapshot:/Users/tsmith/Zotero/storage/49SYASFH/2304.html:text/html;Chattopadhyay_Hassanzadeh_2023_Long-term_instabilities_of_deep_learning-based_digital_twins_of_the_climate.pdf:/Users/tsmith/Drive/zotero/Chattopadhyay_Hassanzadeh_2023_Long-term_instabilities_of_deep_learning-based_digital_twins_of_the_climate.pdf:application/pdf},
}


@article{bi_accurate_2023,
	title = {Accurate medium-range global weather forecasting with {3D} neural networks},
	volume = {619},
	copyright = {2023 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-023-06185-3},
	doi = {10.1038/s41586-023-06185-3},
	language = {en},
	number = {7970},
	journal = {Nature},
	author = {Bi, Kaifeng and Xie, Lingxi and Zhang, Hengheng and Chen, Xin and Gu, Xiaotao and Tian, Qi},
	month = jul,
	year = {2023},
	keywords = {Atmospheric dynamics, Computer science},
	pages = {533--538},
	file = {Bi_et_al_2023_Accurate_medium-range_global_weather_forecasting_with_3D_neural_networks.pdf:/Users/tsmith/Drive/zotero/Bi_et_al_2023_Accurate_medium-range_global_weather_forecasting_with_3D_neural_networks.pdf:application/pdf},
}


@misc{sitzmann_implicit_2020,
	title = {Implicit {Neural} {Representations} with {Periodic} {Activation} {Functions}},
	url = {http://arxiv.org/abs/2006.09661},
	publisher = {arXiv},
	author = {Sitzmann, Vincent and Martel, Julien N. P. and Bergman, Alexander W. and Lindell, David B. and Wetzstein, Gordon},
	month = jun,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv.org Snapshot:/Users/tsmith/Zotero/storage/2K9TJG6C/2006.html:text/html;Sitzmann_et_al_2020_Implicit_Neural_Representations_with_Periodic_Activation_Functions.pdf:/Users/tsmith/Drive/zotero/Sitzmann_et_al_2020_Implicit_Neural_Representations_with_Periodic_Activation_Functions.pdf:application/pdf},
}


@article{smith_temporal_2023,
	title = {Temporal {Subsampling} {Diminishes} {Small} {Spatial} {Scales} in {Recurrent} {Neural} {Network} {Emulators} of {Geophysical} {Turbulence}},
	volume = {15},
	copyright = {© 2023 The Authors. Journal of Advances in Modeling Earth Systems published by Wiley Periodicals LLC on behalf of American Geophysical Union.},
	issn = {1942-2466},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1029/2023MS003792},
	doi = {10.1029/2023MS003792},
	abstract = {The immense computational cost of traditional numerical weather and climate models has sparked the development of machine learning (ML) based emulators. Because ML methods benefit from long records of training data, it is common to use data sets that are temporally subsampled relative to the time steps required for the numerical integration of differential equations. Here, we investigate how this often overlooked processing step affects the quality of an emulator's predictions. We implement two ML architectures from a class of methods called reservoir computing: (a) a form of Nonlinear Vector Autoregression (NVAR), and (b) an Echo State Network (ESN). Despite their simplicity, it is well documented that these architectures excel at predicting low dimensional chaotic dynamics. We are therefore motivated to test these architectures in an idealized setting of predicting high dimensional geophysical turbulence as represented by Surface Quasi-Geostrophic dynamics. In all cases, subsampling the training data consistently leads to an increased bias at small spatial scales that resembles numerical diffusion. Interestingly, the NVAR architecture becomes unstable when the temporal resolution is increased, indicating that the polynomial based interactions are insufficient at capturing the detailed nonlinearities of the turbulent flow. The ESN architecture is found to be more robust, suggesting a benefit to the more expensive but more general structure. Spectral errors are reduced by including a penalty on the kinetic energy density spectrum during training, although the subsampling related errors persist. Future work is warranted to understand how the temporal resolution of training data affects other ML architectures.},
	language = {en},
	number = {12},
	journal = {Journal of Advances in Modeling Earth Systems},
	author = {Smith, Timothy A. and Penny, Stephen G. and Platt, Jason A. and Chen, Tse-Chun},
	year = {2023},
	keywords = {machine learning, recurrent neural networks, numerical weather prediction, geophysical fluid dynamics},
	pages = {e2023MS003792},
}


@article{hoyer_xarray_2017,
	title = {xarray: {N}-{D} labeled {Arrays} and {Datasets} in {Python}},
	volume = {5},
	issn = {2049-9647},
	shorttitle = {xarray},
	url = {https://openresearchsoftware.metajnl.com/articles/10.5334/jors.148},
	doi = {10.5334/jors.148},
	abstract = {The Journal of Open Research Software (JORS) features peer reviewed Software Metapapers describing research software with high reuse potential. We are working with a number of specialist and institutional repositories to ensure that the associated software is professionally archived, preserved, and is openly available. Equally importantly, the software and the papers will be citable, and reuse will be tracked.JORS also publishes full-length research papers that cover different aspects of creating, maintaining and evaluating open source research software. The aim of the section is to promote the dissemination of best practice and experience related to the development and maintenance of reusable, sustainable research software.},
	language = {en-US},
	number = {1},
	urldate = {2024-05-06},
	journal = {Journal of Open Research Software},
	author = {Hoyer, Stephan and Hamman, Joe},
	month = apr,
	year = {2017},
	file = {Hoyer_Hamman_2017_xarray.pdf:/Users/tsmith/Drive/zotero/Hoyer_Hamman_2017_xarray.pdf:application/pdf},
}


@article{jaeger_harnessing_2004,
	title = {Harnessing {Nonlinearity}: {Predicting} {Chaotic} {Systems} and {Saving} {Energy} in {Wireless} {Communication}},
	volume = {304},
	shorttitle = {Harnessing {Nonlinearity}},
	url = {https://www.science.org/doi/10.1126/science.1091277},
	doi = {10.1126/science.1091277},
	abstract = {We present a method for learning nonlinear systems, echo state networks (ESNs). ESNs employ artificial recurrent neural networks in a way that has recently been proposed independently as a learning mechanism in biological brains. The learning method is computationally efficient and easy to use. On a benchmark task of predicting a chaotic time series, accuracy is improved by a factor of 2400 over previous techniques. The potential for engineering applications is illustrated by equalizing a communication channel, where the signal error rate is improved by two orders of magnitude.},
	number = {5667},
	urldate = {2024-05-06},
	journal = {Science},
	author = {Jaeger, Herbert and Haas, Harald},
	month = apr,
	year = {2004},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {78--80},
	file = {Jaeger_Haas_2004_Harnessing_Nonlinearity.pdf:/Users/tsmith/Drive/zotero/Jaeger_Haas_2004_Harnessing_Nonlinearity.pdf:application/pdf},
}


@article{harris_array_2020,
	title = {Array programming with {NumPy}},
	volume = {585},
	copyright = {2020 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-020-2649-2},
	doi = {10.1038/s41586-020-2649-2},
	abstract = {Array programming provides a powerful, compact and expressive syntax for accessing, manipulating and operating on data in vectors, matrices and higher-dimensional arrays. NumPy is the primary array programming library for the Python language. It has an essential role in research analysis pipelines in fields as diverse as physics, chemistry, astronomy, geoscience, biology, psychology, materials science, engineering, finance and economics. For example, in astronomy, NumPy was an important part of the software stack used in the discovery of gravitational waves1 and in the first imaging of a black hole2. Here we review how a few fundamental array concepts lead to a simple and powerful programming paradigm for organizing, exploring and analysing scientific data. NumPy is the foundation upon which the scientific Python ecosystem is constructed. It is so pervasive that several projects, targeting audiences with specialized needs, have developed their own NumPy-like interfaces and array objects. Owing to its central position in the ecosystem, NumPy increasingly acts as an interoperability layer between such array computation libraries and, together with its application programming interface (API), provides a flexible framework to support the next decade of scientific and industrial analysis.},
	language = {en},
	number = {7825},
	urldate = {2024-05-06},
	journal = {Nature},
	author = {Harris, Charles R. and Millman, K. Jarrod and van der Walt, Stéfan J. and Gommers, Ralf and Virtanen, Pauli and Cournapeau, David and Wieser, Eric and Taylor, Julian and Berg, Sebastian and Smith, Nathaniel J. and Kern, Robert and Picus, Matti and Hoyer, Stephan and van Kerkwijk, Marten H. and Brett, Matthew and Haldane, Allan and del Río, Jaime Fernández and Wiebe, Mark and Peterson, Pearu and Gérard-Marchant, Pierre and Sheppard, Kevin and Reddy, Tyler and Weckesser, Warren and Abbasi, Hameer and Gohlke, Christoph and Oliphant, Travis E.},
	month = sep,
	year = {2020},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computational neuroscience, Computational science, Computer science, Software, Solar physics},
	pages = {357--362},
	file = {Harris_et_al_2020_Array_programming_with_NumPy.pdf:/Users/tsmith/Drive/zotero/Harris_et_al_2020_Array_programming_with_NumPy.pdf:application/pdf},
}


@incollection{Trouvain2020,
  doi = {10.1007/978-3-030-61616-8_40},
  url = {https://doi.org/10.1007/978-3-030-61616-8_40},
  year = {2020},
  publisher = {Springer International Publishing},
  pages = {494--505},
  author = {Nathan Trouvain and Luca Pedrelli and Thanh Trung Dinh and Xavier Hinaut},
  title = {{ReservoirPy}: An Efficient and User-Friendly Library to Design Echo State Networks},
  booktitle = {Artificial Neural Networks and Machine Learning {\textendash} {ICANN} 2020}
}

@inproceedings{hyperopt,
    author={J. Bergstra and D. Yamins and D. D. Cox},
    title={Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures},
    year={2013},
    pages={I-115--I-23},
    booktitle={TProc. of the 30th International Conference on Machine Learning (ICML 2013)},
}



@article{arcomano_hybrid_2022,
	title = {A {Hybrid} {Approach} to {Atmospheric} {Modeling} {That} {Combines} {Machine} {Learning} {With} a {Physics}-{Based} {Numerical} {Model}},
	volume = {14},
	issn = {1942-2466},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1029/2021MS002712},
	doi = {10.1029/2021MS002712},
	abstract = {This paper describes an implementation of the combined hybrid-parallel prediction (CHyPP) approach of Wikner et al. (2020), https://doi.org/10.1063/5.0005541 on a low-resolution atmospheric global circulation model (AGCM). The CHyPP approach combines a physics-based numerical model of a dynamical system (e.g., the atmosphere) with a computationally efficient type of machine learning (ML) called reservoir computing to construct a hybrid model. This hybrid atmospheric model produces more accurate forecasts of most atmospheric state variables than the host AGCM for the first 7–8 forecast days, and for even longer times for the temperature and humidity near the earth's surface. It also produces more accurate forecasts than a model based only on ML, or a model that combines linear regression, rather than ML, with the AGCM. The potential of the CHyPP approach for climate research is demonstrated by a 10-year long hybrid model simulation of the atmospheric general circulation, which shows that the hybrid model can simulate the general circulation with substantially smaller systematic errors and more realistic variability than the host AGCM.},
	language = {en},
	number = {3},
	urldate = {2022-06-17},
	journal = {Journal of Advances in Modeling Earth Systems},
	author = {Arcomano, Troy and Szunyogh, Istvan and Wikner, Alexander and Pathak, Jaideep and Hunt, Brian R. and Ott, Edward},
	year = {2022},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1029/2021MS002712},
	pages = {e2021MS002712},
	file = {Arcomano_et_al_2022_A_Hybrid_Approach_to_Atmospheric_Modeling_That_Combines_Machine_Learning_With_a.pdf:/Users/tsmith/Drive/zotero/Arcomano_et_al_2022_A_Hybrid_Approach_to_Atmospheric_Modeling_That_Combines_Machine_Learning_With_a.pdf:application/pdf;Snapshot:/Users/tsmith/Zotero/storage/4FCCNJQF/2021MS002712.html:text/html},
}

@article{arcomano_hybrid_2023,
	title = {A {Hybrid} {Atmospheric} {Model} {Incorporating} {Machine} {Learning} {Can} {Capture} {Dynamical} {Processes} {Not} {Captured} by {Its} {Physics}-{Based} {Component}},
	volume = {50},
	issn = {1944-8007},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1029/2022GL102649},
	doi = {10.1029/2022GL102649},
	abstract = {It is shown that a recently developed hybrid modeling approach that combines machine learning (ML) with an atmospheric global circulation model (AGCM) can serve as a basis for capturing atmospheric processes not captured by the AGCM. This power of the approach is illustrated by three examples from a decades-long climate simulation experiment. The first example demonstrates that the hybrid model can produce sudden stratospheric warming, a dynamical process of nature not resolved by the low resolution AGCM component of the hybrid model. The second and third example show that introducing 6-hr cumulative precipitation and sea surface temperature (SST) as ML-based prognostic variables improves the precipitation climatology and leads to a realistic ENSO signal in the SST and atmospheric surface pressure.},
	language = {en},
	number = {8},
	urldate = {2023-05-02},
	journal = {Geophysical Research Letters},
	author = {Arcomano, Troy and Szunyogh, Istvan and Wikner, Alexander and Hunt, Brian R. and Ott, Edward},
	year = {2023},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1029/2022GL102649},
	keywords = {machine learning, climate, numerical weather prediction},
	pages = {e2022GL102649},
	file = {Arcomano_et_al_2023_A_Hybrid_Atmospheric_Model_Incorporating_Machine_Learning_Can_Capture_Dynamical.pdf:/Users/tsmith/Drive/zotero/Arcomano_et_al_2023_A_Hybrid_Atmospheric_Model_Incorporating_Machine_Learning_Can_Capture_Dynamical.pdf:application/pdf;Snapshot:/Users/tsmith/Zotero/storage/GTX98CFJ/2022GL102649.html:text/html},
}


@misc{arcomano_code,
	title = {Arcomano1234/{SPEEDY}-{ML}: {V1} - {GRL} {Paper}},
	url = {https://doi.org/10.5281/zenodo.7508156},
	publisher = {Zenodo},
	author = {Arcomano, Troy},
	month = jan,
	year = {2023},
	doi = {10.5281/zenodo.7508156},
}

@article{liaw2018tune,
    title={Tune: A Research Platform for Distributed Model Selection and Training},
    author={Liaw, Richard and Liang, Eric and Nishihara, Robert
            and Moritz, Philipp and Gonzalez, Joseph E and Stoica, Ion},
    journal={arXiv preprint arXiv:1807.05118},
    year={2018}
}

@software{jax2018github,
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url = {http://github.com/google/jax},
  version = {0.3.13},
  year = {2018},
}
